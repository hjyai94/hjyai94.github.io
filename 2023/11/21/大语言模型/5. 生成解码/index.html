<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>

<script>
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误');
                history.back();
            }
        }
    })();
</script>

<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />










  <meta name="baidu-site-verification" content="i0w0IGpafr" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="LLM," />










<meta name="description" content="自回归语言生成我们先回顾一下自回归 (auto-regressive) 语言生成的过程。自回归语言生成假设每个词语序列的概率都可以分解为一系列条件词语概率的乘积:$$P\left(w_{1: T} \mid W_0\right)&#x3D;\prod_{t&#x3D;1}^T P\left(w_t \mid w_{1: t-1}, W_0\right), \quad w_{1: 0}&#x3D;\">
<meta property="og:type" content="article">
<meta property="og:title" content="生成解码">
<meta property="og:url" content="http://hjyai94.cn/2023/11/21/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/5.%20%E7%94%9F%E6%88%90%E8%A7%A3%E7%A0%81/index.html">
<meta property="og:site_name" content="HJY">
<meta property="og:description" content="自回归语言生成我们先回顾一下自回归 (auto-regressive) 语言生成的过程。自回归语言生成假设每个词语序列的概率都可以分解为一系列条件词语概率的乘积:$$P\left(w_{1: T} \mid W_0\right)&#x3D;\prod_{t&#x3D;1}^T P\left(w_t \mid w_{1: t-1}, W_0\right), \quad w_{1: 0}&#x3D;\">
<meta property="og:locale">
<meta property="og:image" content="https://transformers.run/assets/img/transformers-note-7/human_text_vs_beam_search.png">
<meta property="article:published_time" content="2023-11-21T06:13:00.000Z">
<meta property="article:modified_time" content="2023-12-19T09:34:24.160Z">
<meta property="article:author" content="HJY">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://transformers.run/assets/img/transformers-note-7/human_text_vs_beam_search.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://hjyai94.cn/2023/11/21/大语言模型/5. 生成解码/"/>





  <title>生成解码 | HJY</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="/custom_css_source.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">HJY</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">昨夜西风凋碧树，独上高楼，望尽天涯路。</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hjyai94.cn/2023/11/21/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/5.%20%E7%94%9F%E6%88%90%E8%A7%A3%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">生成解码</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-11-21T14:13:00+08:00">
                2023-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="自回归语言生成"><a href="#自回归语言生成" class="headerlink" title="自回归语言生成"></a>自回归语言生成</h1><p>我们先回顾一下自回归 (auto-regressive) 语言生成的过程。自回归语言生成假设每个词语序列的概率都可以分解为一系列条件词语概率的乘积:<br>$$<br>P\left(w_{1: T} \mid W_0\right)&#x3D;\prod_{t&#x3D;1}^T P\left(w_t \mid w_{1: t-1}, W_0\right), \quad w_{1: 0}&#x3D;\varnothing<br>$$</p>
<p>这样就可以迭代地基于上下文 $W_0$ 以及已经生成的词语序列 $w_{1: t-1}$ 来预测序列中的下一个词 $w_t$ ，因此被称为自回归 (auto-regressive)。生成序列的长度 $T$ 通常不是预先确定的，而是当生成出休止符（EOS token）时结束迭代。<br>Transformers 库中所有的生成模型都提供了用于自回归生成的 <code>generate()</code> 函数，例如 GPT2、XLNet、OpenAi-GPT、CTRL、TransfoXL、XLM、Bart、T5 等等。<br>下面我们将介绍目前常用的四种解码方式：</p>
<ul>
<li>贪心搜索 (Greedy Search)</li>
<li>柱搜索 (Beam search)</li>
<li><em>Top-K</em> 采样 (<em>Top-K</em> sampling)</li>
<li><em>Top-p</em> 采样 (<em>Top-p</em> sampling)。</li>
</ul>
<p>为了方便，我们将统一使用 GPT-2 模型来进行展示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add the EOS token as PAD token to avoid warnings</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;gpt2&quot;</span>, pad_token_id=tokenizer.eos_token_id)</span><br></pre></td></tr></table></figure>

<h2 id="贪心搜索"><a href="#贪心搜索" class="headerlink" title="贪心搜索"></a>贪心搜索</h2><p>贪心搜索 (Greedy Search) 在每轮迭代时，即在时间步 $t$ ，简单地选择概率最高的下一个词作为当前词，即 $w_t&#x3D;\operatorname{argmax}<em>w P\left(w \mid w</em>{1: t-1}\right)$ 。下图展示了一个贪心搜索的例子:<br>![[Pasted image 20231121141847.png|500]]<br>可以看到，从起始词语“The”开始，贪心算法不断地选择概率最高的下一个词直至结束，最后生成词语序列 (“The” “nice” “woman”)，其整体概率为 $0.5×0.4&#x3D;0.2$。</p>
<p>下面我们使用 GPT-2 模型结合贪心算法来为上下文 (“I”, “enjoy”, “walking”, “with”, “my”, “cute”, “dog”) 生成后续序列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encode context the generation is conditioned on</span></span><br><span class="line">input_ids = tokenizer.encode(<span class="string">&#x27;I enjoy walking with my cute dog&#x27;</span>, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># generate text until the output length (which includes the context length) reaches 50</span></span><br><span class="line">greedy_output = model.generate(input_ids, max_length=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(greedy_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking <span class="keyword">with</span> my cute dog, but I<span class="string">&#x27;m not sure if I&#x27;</span>ll ever be able to walk <span class="keyword">with</span> my dog. I<span class="string">&#x27;m not sure if I&#x27;</span>ll ever be able to walk <span class="keyword">with</span> my dog.</span><br><span class="line"></span><br><span class="line">I<span class="string">&#x27;m not sure if I&#x27;</span>ll</span><br></pre></td></tr></table></figure>

<p>模型成功地生成了一个短文本，但是它似乎开始不停地重复。这是一个语言生成中常见的问题，特别是在贪心搜索和柱搜索中经常会出现。</p>
<p><strong>贪心搜索最大的问题是由于每次都只选择当前概率最大的词，相当于是区部最优解，因此生成的序列往往并不是全局最优的。</strong> 例如在上图中，词语序列 (“The”, “dog”, “has”) 的概率是 $0.4×0.9&#x3D;0.36$，而这个序列无法通过贪心算法得到。</p>
<h2 id="柱搜索"><a href="#柱搜索" class="headerlink" title="柱搜索"></a>柱搜索</h2><p>柱搜索 (Beam search) 在每个时间步都保留 num_beams 个最可能的词，最终选择整体概率最大的序列作为结果。下图展示了一个 <code>num_beams=2</code> 的例子：<br>![[Pasted image 20231121144829.png|500]]</p>
<p>可以看到，在第一个时间步，柱搜索同时保留了概率最大的前 2 个序列：概率为 0.4 的 (”The“, ”dog“) 和概率为 0.5 的 (”The“, ”nice“)；在第二个时间步，柱搜索通过计算继续保留概率最大的前 2 个序列：概率为 0.4×0.9&#x3D;0.36 的 (”The“, ”dog“, ”has“) 和概率为 0.5×0.4&#x3D;0.2 的 (”The“, ”nice“, ”woman“)；最终选择概率最大的序列 (”The“, ”dog“, ”has“) 作为结果。</p>
<blockquote>
<p>柱搜索虽然通过在每个时间步保留多个分支来缓解贪心算法局部最优解的问题，但是它依然不能保证找到全局最优解。</p>
</blockquote>
<p>下面我们同样运用 GPT-2 模型结合柱搜索来生成文本，只需要设置参数 <code>num_beams &gt; 1</code> 以及 <code>early_stopping=True</code>，这样只要所有柱搜索保留的分支都到达休止符 EOS token，生成过程就结束。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># activate beam search and early_stopping</span></span><br><span class="line">beam_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    num_beams=<span class="number">5</span>, </span><br><span class="line">    early_stopping=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(beam_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking <span class="keyword">with</span> my cute dog, but I<span class="string">&#x27;m not sure if I&#x27;</span>ll ever be able to walk <span class="keyword">with</span> him again.</span><br><span class="line"></span><br><span class="line">I<span class="string">&#x27;m not sure if I&#x27;</span>ll ever be able to walk <span class="keyword">with</span> him again. I<span class="string">&#x27;m not sure if I&#x27;</span>ll</span><br></pre></td></tr></table></figure>

<p>虽然柱搜索得到的序列更加流畅，但是输出中依然出现了重复片段。最简单的解决方法是引入 n-grams 惩罚，其在每个时间步都手工将那些会产生重复 n-gram 片段的词的概率设为 0。例如，我们额外设置参数 <code>no_repeat_ngram_size=2</code> 就能使生成序列中不会出现重复的 2-gram 片段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set no_repeat_ngram_size to 2</span></span><br><span class="line">beam_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    num_beams=<span class="number">5</span>, </span><br><span class="line">    no_repeat_ngram_size=<span class="number">2</span>, </span><br><span class="line">    early_stopping=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(beam_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking <span class="keyword">with</span> my cute dog, but I<span class="string">&#x27;m not sure if I&#x27;</span>ll ever be able to walk <span class="keyword">with</span> him again.</span><br><span class="line"></span><br><span class="line">I<span class="string">&#x27;ve been thinking about this for a while now, and I think it&#x27;</span>s time <span class="keyword">for</span> me to take a <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>不过 n-grams 惩罚虽然能够缓解“重复”问题，却也要谨慎使用。例如对于一篇关于”New York“文章就不能使用 2-gram 惩罚，否则”New York“在全文中就只能出现一次了。</p>
<p>柱搜索会在每个时间步都保留当前概率最高的前 num_beams 个序列，因此我们还可以通过设置参数 <code>num_return_sequences</code>（&lt;&#x3D; num_beams）来返回概率靠前的多个序列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set return_num_sequences &gt; 1</span></span><br><span class="line">beam_outputs = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    num_beams=<span class="number">5</span>, </span><br><span class="line">    no_repeat_ngram_size=<span class="number">2</span>, </span><br><span class="line">    num_return_sequences=<span class="number">3</span>, </span><br><span class="line">    early_stopping=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># now we have 3 output sequences</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i, beam_output <span class="keyword">in</span> <span class="built_in">enumerate</span>(beam_outputs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;\n\n&quot;</span>.<span class="built_in">format</span>(i, tokenizer.decode(beam_output, skip_special_tokens=<span class="literal">True</span>)))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line"><span class="number">0</span>: I enjoy walking <span class="keyword">with</span> my cute dog, but I<span class="string">&#x27;m not sure if I&#x27;</span>ll ever be able to walk <span class="keyword">with</span> him again.</span><br><span class="line"></span><br><span class="line">I<span class="string">&#x27;ve been thinking about this for a while now, and I think it&#x27;</span>s time <span class="keyword">for</span> me to take a <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>: I enjoy walking <span class="keyword">with</span> my cute dog, but I<span class="string">&#x27;m not sure if I&#x27;</span>ll ever be able to walk <span class="keyword">with</span> him again.</span><br><span class="line"></span><br><span class="line">I<span class="string">&#x27;ve been thinking about this for a while now, and I think it&#x27;</span>s time <span class="keyword">for</span> me to get back to</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">2</span>: I enjoy walking <span class="keyword">with</span> my cute dog, but I<span class="string">&#x27;m not sure if I&#x27;</span>ll ever be able to walk <span class="keyword">with</span> her again.</span><br><span class="line"></span><br><span class="line">I<span class="string">&#x27;ve been thinking about this for a while now, and I think it&#x27;</span>s time <span class="keyword">for</span> me to take a <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>由于柱大小只被设为 5，因此最终获得的 3 个序列看上去非常接近。</p>
<p>有趣的是，人类语言似乎并不遵循下一个词是最高概率的分布，换句话说，真实的人类语言具有高度的随机性，是不可预测的。下图展示了人类语言与柱搜索在每个时间步词语概率的对比：</p>
<p><img src="https://transformers.run/assets/img/transformers-note-7/human_text_vs_beam_search.png"></p>
<p>因此，柱搜索更适用于机器翻译或摘要等生成序列长度大致可预测的任务，而在对话生成、故事生成等开放式文本生成任务 (open-ended generation) 上表现不佳。虽然通过 n-gram 或者其他惩罚项可以缓解“重复”问题，但是如何控制”不重复”和“重复”之间的平衡又非常困难。</p>
<p>所以，对于开放式文本生成任务，我们需要引入更多的随机性——这就是采样方法。</p>
<h2 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h2><p>采样 (sampling) 最基本的形式就是从当前上下文的条件概率分布中随机地选择一个词作为下一个词，即：<br>$$<br>w_t \sim P\left(w \mid w_{1: t-1}\right)<br>$$<br>对于前面图中的例子，一个基于采样的生成过程可能为（采样生成的结果不是唯一的）：<br>![[Pasted image 20231121150710.png]]</p>
<p>这里“car”是从条件概率分布 $P(w \mid$ “The” $)$ 中采样得到，而“drives”是从分布 $P(w \mid$ “The”, “car” ) 中采样得到。<br>在 Transformers 库中，我们只需要在 generate() 中设置 do_sample&#x3D;True 并且令 top_k&#x3D;0 禁用 Top- $K$ 采样就可以实现随机采样:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># activate sampling and deactivate top_k by setting top_k sampling to 0</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_k=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking <span class="keyword">with</span> my cute dog along the seven mile loop along Green Bay<span class="string">&#x27;s Skagit River. In the central part of Monroe County about 100 miles east of sheboygan, it is almost deserted. But along the way there are often carefully</span></span><br></pre></td></tr></table></figure>

<p>看上去还不错，但是细读的话会发现不是很连贯，这也是采样生成文本的通病：模型经常会生成前后不连贯的片段。一种解决方式是通过降低 softmax 的温度 (temperature) 使得分布 $P\left(w \mid w_{1: t-1}\right)$ 更尖锐，即进一步增加高概率词出现的可能性和降低低概率词出现的可能性。例如对上面的例子应用降温:<br>![[Pasted image 20231121151133.png]]</p>
<p>这样在第一个时间步，条件概率变得更加尖锐，几乎不可能会选择到“car”。我们只需要在 <code>generate()</code> 中设置 <code>temperature</code> 来就可以实现对分布的降温：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use temperature to decrease the sensitivity to low probability candidates</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_k=<span class="number">0</span>, </span><br><span class="line">    temperature=<span class="number">0.6</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking <span class="keyword">with</span> my cute dog, but it<span class="string">&#x27;s pretty much impossible to get the best out of my dog.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Pinky is a bit of a big she-wolf, but she is pretty much the most adorable of all the wolves.</span></span><br></pre></td></tr></table></figure>

<p>可以看到生成的文本更加连贯了。<strong>降温操作实际上是在减少分布的随机性，当我们把 temperature 设为 0 时就等同于贪心解码。</strong></p>
<h2 id="Top-K-采样"><a href="#Top-K-采样" class="headerlink" title="Top-K 采样"></a>Top-K 采样</h2><p>类似于柱搜索，<strong>Top- $K$ 采样在每个时间步都保留最可能的 $K$ 个词，然后在这 $K$ 个词上重新分配概率质量。</strong> 例如我们对上面的示例进行 Top- $K$ 采样，这里设置 $K&#x3D;6$ 在每个时间步都将采样池控制在 6 个词。<br>![[Pasted image 20231121151332.png]]<br>可以看到，6 个最可能的词（记为 $V_{\mathrm{top}-\mathrm{K}}$ ）虽然仅包含第一个时间步中整体概率质量的大约 $\frac{2}{3}$ ，但是几乎包含了第二个时间步中所有的概率质量。尽管如此，它还是成功地消除了第二步中那些奇怪的候选词（例如”not”、”the”、”small”、”told” )。</p>
<p>下面我们通过在 <code>generate()</code> 中设置 <code>top_k=10</code> 来进行 <em>Top-K</em> 采样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set top_k to 10</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_k=<span class="number">10</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking <span class="keyword">with</span> my cute dog, but it<span class="string">&#x27;s a bit of a pain in the ass to see that the dog does not get to walk with me.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">I think my dog is just fine. But he needs some time to get used</span></span><br></pre></td></tr></table></figure>

<p><em>Top-K</em> 采样的一个问题是它无法动态调整每个时间步从概率分布 $P$ 中过滤出来的单词数量，这会导致有些词可能是从<strong>非常尖锐的分布</strong>（上图中右侧）中采样的，而其他单词则可能是从平坦的分布（上图中左侧）中采样的，从而无法保证生成序列整体的质量。</p>
<h2 id="Top-p-nucleus-采样"><a href="#Top-p-nucleus-采样" class="headerlink" title="Top-p (nucleus) 采样"></a>Top-p (nucleus) 采样</h2><p><strong><em>Top-p</em> 对 <em>Top-K</em> 进行了改进，每次只从累积概率超过 $p$ 的最小的可能词集中进行选择，然后在这组词语中重新分配概率质量。</strong> 这样，每个时间步的词语集合的大小就可以根据下一个词的条件概率分布动态增加和减少。下图展示了一个 Top-p 采样的例子：<br>![[Pasted image 20231121152000.png]]</p>
<p>这里我们设置 $p&#x3D;0.92$ ，$Top-p$采样在每个时间步会在整体概率质量超过 $92 %$ 的最小单词集合（定义为 $V_{\text {top-p }}$ ）中进行选择。上图左边的例子中，Top- $p$ 采样出了 9 个最可能的词语，而在右边的例子中，只选了 3 个最可能的词，整体概率质量就已经超过了 $92 %$ 。可以看到，当下一个词难以预测时（例如 $P(w \mid “The”)$ ），Top-p 采样会保留很多可能的词，而当下一个词相对容易预测时（例如 $P(w \mid “The”, “car”)$)，Top-p 就只会保留很少的词。</p>
<p>我们只需要在 generate () 中设置 $\theta&lt;t o p _p&lt;1$ 就可以激活 Top-p 采样了:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># deactivate top_k sampling and sample only from 92% most likely words</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_p=<span class="number">0.92</span>, </span><br><span class="line">    top_k=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking <span class="keyword">with</span> my cute dog along the Tokyo highway,<span class="string">&quot; said Beranito, 47, the man who moved to the new apartment in 2013 with his wife. &quot;</span>I liked to sit <span class="built_in">next</span> to him on the roof when I was doing programming.</span><br></pre></td></tr></table></figure>

<p>虽然理论上 <em>Top-p</em> 采样比 <em>Top-K</em> 采样更灵活，但是两者在实际应用中都被广泛采用，<em>Top-p</em> 甚至可以与 <em>Top-K</em> 共同工作，这可以在排除低概率词的同时还允许进行一些动态选择。</p>
<p>最后，与贪心搜索类似，为了获得多个独立采样的结果，我们设置 <code>num_return_sequences &gt; 1</code>，并且同时结合 <em>Top-p</em> 和 <em>Top-K</em> 采样：</p>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">
      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>非常感谢各位老板投喂！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/%E5%BE%AE%E4%BF%A1%E6%89%93%E8%B5%8F.png" alt=" 微信支付"/>
        <p>微信支付</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/LLM/" rel="tag"># LLM</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2023/11/20/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/3.%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" rel="next" title="注意力机制">
                <i class="fa fa-chevron-left"></i> 注意力机制
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2023/11/21/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/4.%20Transformers%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%86%E7%B1%BB/" rel="prev" title="4. Transformers的模型与分类">
                4. Transformers的模型与分类 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">82</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/hjyai94" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90"><span class="nav-number">1.</span> <span class="nav-text">自回归语言生成</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%AA%E5%BF%83%E6%90%9C%E7%B4%A2"><span class="nav-number">1.1.</span> <span class="nav-text">贪心搜索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%B1%E6%90%9C%E7%B4%A2"><span class="nav-number">1.2.</span> <span class="nav-text">柱搜索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7"><span class="nav-number">1.3.</span> <span class="nav-text">随机采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Top-K-%E9%87%87%E6%A0%B7"><span class="nav-number">1.4.</span> <span class="nav-text">Top-K 采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Top-p-nucleus-%E9%87%87%E6%A0%B7"><span class="nav-number">1.5.</span> <span class="nav-text">Top-p (nucleus) 采样</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HJY</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.3</div>






        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.3"></script>



  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
