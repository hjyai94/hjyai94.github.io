<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />










  <meta name="baidu-site-verification" content="i0w0IGpafr" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="HJY的装逼小站">
<meta property="og:type" content="website">
<meta property="og:title" content="HJY">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="HJY">
<meta property="og:description" content="HJY的装逼小站">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HJY">
<meta name="twitter:description" content="HJY的装逼小站">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: 'HR62QHMRVP',
      apiKey: '5003cc57039452aa0e152bdb9198ed17',
      indexName: 'dev_NAME',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/3/"/>





  <title>HJY</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">HJY</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">昨夜西风凋碧树，独上高楼望尽天涯路。</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/10/高斯图模型图和伊辛图模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/10/高斯图模型图和伊辛图模型/" itemprop="url">高斯图模型图和伊辛图模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-10T11:01:03+08:00">
                2018-06-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>不同于最大似然估计对于贝叶斯网的估计，有向图中，网络结构通常是已知的，我们需要做的是将参数学习出来或者是对于变量进行推断。无向图中则并不是这样，无向图中，很多模型的结构并不是完全清楚的，需要我们队模型结构进行推断。</p>
<h1 id="高斯图模型"><a href="#高斯图模型" class="headerlink" title="高斯图模型"></a>高斯图模型</h1><p>高斯图模型是马尔科夫随机场的成对形式，同样也是满足高斯正态分布：<br>$$ p(x\mid \mu, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp[-\frac{1}{2}(x-\mu)^T \Sigma^{-T}(x-\mu)] $$<br>其中$\mu$是均值，$\Sigma$是协方差矩阵。令$\mu=0$和精度矩阵为$Q=\Sigma^{-1}$，有：<br>$$ p(x_1, x_2, …, x_p\mid \mu=0, Q) = \frac{|Q|^{1/2}}{(2\pi)^{n/2}}exp[-\frac{1}{2}\sum_i q_{ii}(x_i)^2 - \sum_{i&lt;j}q_{ij}x_ix_j] $$<br>这就是条件随机场，定义于成对边和节点上。</p>
<h1 id="协方差矩阵与精度矩阵"><a href="#协方差矩阵与精度矩阵" class="headerlink" title="协方差矩阵与精度矩阵"></a>协方差矩阵与精度矩阵</h1><p>协方差矩阵有一个重要的性质是：当$\Sigma_{i,j}=0$有$x_i\perp x_j$；逆协方差矩阵（精度矩阵）的对应的性质为：当$\Sigma_{i,j}^{-1}=0$时$x_i\perp x_j\mid x_{-ij}$。</p>
<h1 id="利用LASSO进行网络学习"><a href="#利用LASSO进行网络学习" class="headerlink" title="利用LASSO进行网络学习"></a>利用LASSO进行网络学习</h1><h2 id="LASSO回归"><a href="#LASSO回归" class="headerlink" title="LASSO回归"></a>LASSO回归</h2><p>对于网络结构的学习，我们通常是假设网络是稀疏的。LASSO回归可以用于网络的近邻选择，去除不必要的节点之间的连接。<br>$$\hat{\beta_1} = argmin_{\beta_1}\parallel Y - X\beta_1\parallel^2 + \lambda\parallel\beta_1\parallel_1$$<br>其中，$\beta_1$是节点1的参数，Y是是对节点1的独立观测值。<br>具体过程如图所示：<br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/LASSO%E5%9B%9E%E5%BD%92.png" alt=""></p>
<h2 id="理论条件"><a href="#理论条件" class="headerlink" title="理论条件"></a>理论条件</h2><ul>
<li>Dependency Condition: Relevant Covariates are not overly dependent</li>
<li>Incoherence Condition: Large number of irrelevant covariates can’t be too correlated with relevant covariates</li>
<li>Strong concentration bounds: Sample quantities coverge to expected values quickly</li>
</ul>
<h1 id="时变网络"><a href="#时变网络" class="headerlink" title="时变网络"></a>时变网络</h1><h2 id="KELLER-Kernel-Weightd-L-1-regularized-logistic-Regression"><a href="#KELLER-Kernel-Weightd-L-1-regularized-logistic-Regression" class="headerlink" title="KELLER: Kernel Weightd $L_1$-regularized logistic Regression"></a>KELLER: Kernel Weightd $L_1$-regularized logistic Regression</h2><p>对时变网络的结构进行估计，可以采用KELLER的方法解决：<br>$$ \hat{\theta_i^t} = argmain_{\theta_i^t}l_w(\theta_i^t) + \lambda_1\parallel\theta_t^t \parallel_1 $$<br>其中$l_w(\theta_i^t) = \sum_{t’=1}^T w(x^{t’}; x^t)log\ P(x_i^{t’}\mid x_{-i} x^{t’}, \theta_i^t)$。权值$w(x^{t’}; x^t)$决定了在时间$t’$和$t$之间的关系，我们可以将其建模为一个分布(如下图)。<br><img src="" alt=""><br>给定时间$t^{\ast}$，权值可以写成：<br>$$ w_t(t^{\ast}) = \frac{K_{h_n}(t-t^{\ast})}{\sigma_{t’\in T^n} K_{h_n}(t’-t^{\ast})} $$<br>对于一些平滑的核$K_{h_n}$。</p>
<h2 id="TESLA-Temporally-Smoothed-L-1-regularized-logistic-regression"><a href="#TESLA-Temporally-Smoothed-L-1-regularized-logistic-regression" class="headerlink" title="TESLA: Temporally Smoothed $L_1$-regularized logistic regression"></a>TESLA: Temporally Smoothed $L_1$-regularized logistic regression</h2><p>TESLA对于一个节点的参数优化是基于所有的时间步的：<br>$$ \hat{\theta_i^T}, …, \hat{\theta_i^T} = argmin\sum_{i=1}^T l_{avg}(\theta_i^t) + \lambda_1 \sum_{t=1}^T \parallel\theta_{-1}^t \parallel_1 + \lambda_2\sum_{t=1}^T \parallel \theta_i^t - \theta_i^{t-1} \parallel $$<br>其中，$l_{avg}(\theta_i^t) = \frac{1}{N^t}\sum_{d=1}^{N^t} log\ P(x_{d,i}^t\mid x_{d, -i}^t, \theta_i^t) $是条件对数似然。不同于KELLER，当节点数达到5000时，这里我们不需要平滑Kernels，这里可以接受突变。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/31/HMM和CRF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/31/HMM和CRF/" itemprop="url">HMM和CRF</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-31T09:49:19+08:00">
                2018-05-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h1><p>隐马尔可夫模型如下图所示：<br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B.png" alt=""></p>
<h2 id="公式表达"><a href="#公式表达" class="headerlink" title="公式表达"></a>公式表达</h2><p>对于隐马尔可夫模型，通常有三组参数：<br>$$ trasition\ probability\ matrix\ A: p(y_t^j = 1\mid y_{t-1}^i=1)=a_{i,j} $$   $$ initial\ probability: p(y_1)\sim Multinomial(\pi_1, \pi_2, …, \pi_M) $$    $$ emission\ probabilies: p(x_t\mid y_t^i)\sim Multinomial(b_{i,1}, b_{i,2}, …, b_{i,K}) $$</p>
<h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><ul>
<li><p>前向算法<br>$$ \alpha_t^k \equiv\mu_{t-1\rightarrow}(k)=P(x_1, x_2, …, x_t, y_t^k=1) $$<br>$$ \alpha_t^k = p(x_t\mid y_t^k=1)\sum_i \alpha_{t-1}^ia_{i,k} $$</p>
</li>
<li><p>后向算法<br>$$ \beta_t^k \equiv \mu_{t\leftarrow t+1}(k)=P(x_{t+1}, …, x_T\mid y_t^k=1) $$<br>$$ \beta_t^k = \sum_i a_{k,i}p(x_{t+1}\mid y_{t+1}^i = 1)\beta_{t+1}^i $$</p>
</li>
</ul>
<p>对于给定观测值下的任意隐变量状态，可以同过点乘前向和后向信息得到。<br>$$ \gamma_t^i = p(y_t^i = 1\mid x_{1:T})\propto \alpha_t^i\beta_t^i =\sum_j \xi_t^{i,j} $$<br>其中有定义：<br>\begin{equation}\begin{split} \xi_t^{i,j} &amp;= p(y_t^i=1,y_{t-1}^j=1, x_{1:T}) \\<br>&amp;\propto \mu_{t-1\rightarrow t}(y_t^i=1)\mu_{t\leftarrow t+1}(y_{t+1}^i=1)p(x_{x+1}\mid y_{t+1})p(y_{t+1}\mid y_t) \\<br>&amp;= \alpha_t^i\beta_{t+1}^j a_{i,j} p(x_{t+1}\mid y_{t+1}^i=1) \\<br>\end{split}\end{equation}<br>具体推导可以参考Youtube上徐亦达老师关于HMM的视频，主要思路就是message passing，运用一些迭代地技巧，可以先从最小的下标开始推导，这样比较容易发现规律，类似于数学归纳法。<br>在Matlab中可以将公式用向量表示，这样方便处理。<br>\begin{equation}\begin{split} &amp;B_t(i)=p(x_t\mid y_t^i=1)\\<br>&amp; A(i,j)=p(y_{t+1}^j=1\mid y_t^i=1) \\<br>&amp; \alpha_t = (A^T\alpha_{t-1}).\ast B_t \\<br>&amp; \beta_t = A(\beta_{t+1}.\ast B_{t+1}) \\<br>&amp; \xi_t = (\alpha_t(\beta_{t+1}.\ast B_{t+1})^T).\ast A  \\<br>&amp; \gamma_t = \alpha_t.\ast \beta_t \\<br>\end{split}\end{equation}</p>
<h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>当我们知道实际状态路径时，监督学习并不是一件困难的事情(trival)，我们只需要数出转移概率和发射概率的实例就可以得到最大似然估计。<br>$$ a_{ij}^{ML} = \frac{\sum_n\sum_{t=2}^T y_{n,t-1}^i y_{n,t}^j}{\sum_n\sum_{t=2}^T y_{n,t-1}^i} $$<br>$$ b_{ik}^{ML} = \frac{\sum_n\sum_{t=2}^T y_{n,t}^i x_{n,t}^k}{\sum_n\sum_{t=2}^T y_{n,t}^i} $$<br>使用了伪计数的方式，可以避免零概率的出现。对于不是多项分布的情况，特别是高斯分布，我们可以利用采样的方法计算均值和方差。</p>
<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>当隐状态不可观的时候，可以使用Baum Welch算法进行处理完全对数似然函数，这一算法就是EM算法对HMM的求解方法。似然函数可以写成：<br>$$ l_c(\theta;x,y)=lop\ p(x,y)=log\prod_n (p(y_{n,1})\prod_{t=1}^T p(y_{n,t}\mid y_{n,t-1})\prod_{t=1}^T p(x_{n,t}\mid y_{n,t})) $$<br>完全对数似然期望是：<br>$$ \langle l_c(\theta;x,y)\rangle = \sum_n (\langle y_{n,1}^i \rangle_{p(y_{n,1}\mid x_n)}log\ \pi_i) + \sum_n\sum_{t=2}^T (\langle y_{n,t-1}^i y_{n,t}^j\rangle_{p(y_{n,t-1},y_{n,t}\mid x_n)}log\ a_{i,j}) + \sum_n\sum_{t=1}^T (x_{n,t}^k\langle y_{n,t}^i\rangle_{p(y_{n,t}\mid x_n)}log\ b_{i,k}) $$</p>
<ul>
<li>E步：<br>$$ \gamma_{n,t}^i = \langle  y_{n,t}^i\rangle = p(y_{n,t}^i = 1\mid x_n) $$    $$ \xi_{n,t}^{i,j} = \langle y_{n,t-1}^i y_{n,t}^j\rangle = p(y_{n,t-1}^i=1, y_{n,t}^j=1\mid x_n) $$</li>
<li>M步：<br>$$ \pi_i=\frac{\sum_n \gamma_{n,1}^i}{N}, a_{i,j}=\frac{\sum_n\xi_{n,t}^{i,j}} {\sum_n\sum_{t=1}^{i,j} \gamma_{n,t}^i},  b_{ik}=\frac{\sum_N\sum_{t=1}^T \gamma_{n,t}^i x_{n,t}^k} {\sum_n\sum_{t=1}^{T-1}\gamma_{n,t}^i} $$</li>
</ul>
<h3 id="HMM的缺点"><a href="#HMM的缺点" class="headerlink" title="HMM的缺点"></a>HMM的缺点</h3><p>HMM的缺点也是HMM的最要特征，就是每个观测值只与一个隐状态相关，与其他状态都无关。另外就是预测目标函数与学习目标函数不一致，HMM学习状态和观测值的联合概率$P(Y,X)$，但是我们的预测要求是需要条件概率$P(Y\mid X)$，通过这样的考虑，有了一个新的模型MEMM。</p>
<h1 id="MEMM"><a href="#MEMM" class="headerlink" title="MEMM"></a>MEMM</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>MEMM结构如下图所示：<br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/MEMM.png" alt=""><br>MEMM的主要特点是，模型中的每个状态都与所有的观测值相关，同时模型是一个判别模型。<br>$$ P(y_{1:n}\mid x_{1:n}) = \prod_{i=1}^{n} P(y_i\mid y_{i-1},x_{1:n}) = \prod_{i=1}^n \frac{exp(W^T f(y_i,y_{i-1}, x_{1:n}))}{Z(y_{i-1}, x_{1:n})} $$</p>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>MEMM存在着标注偏置的问题(label bias preblem)，主要是因为状态转移的路径多少的问题，MEMM中的状态倾向于转移到转移状态路径少的状态，因为转移路径少的状态总能提供较大的转移概率。</p>
<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1><p>一个比较好的方法解决上面的问题就是改变原来的概率转移的方式，用势函数取代概率来表征局部的信息。</p>
<h2 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h2><p>CRF结构如下图所示：<br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/CRF.png" alt=""><br>\begin{equation}\begin{split} P(Y\mid X) &amp;= \frac{1}{Z(X)}\prod_{i=1}^n \phi(y_i,y_{i-1},X) \\<br>&amp;= \frac{1}{Z(X, \lambda, \mu)}exp(\sum_{i=1}^T(\sum_k \lambda_k f_k(y_i, y_{i-1}, X) + \sum_l \mu_l g_l(y_i, X))) \\<br>\end{split}\end{equation}<br>其中，$Z(X,\lambda, \mu)=\sum_y exp(\sum_{i=1}(\sum_k \lambda_k f_k(y_i, y_{i-1}, X) + \sum_l \mu_l g_l (y_i, X))) $，可以看出其中的正规因子是全局的，并不是局部的，这样就保证了对局部信息的处理具有全局一致性。</p>
<p>##　推断<br>所谓的推断问题就是在CRF给定参数$\lambda$和$\mu$，我们可以找到$y^{\ast}$使得$P(y\mid x)$最大。<br>$$ y^{\ast} = argmax_y exp(\sum_{i=1}^n (\sum_k \lambda_k f_k (y_i, y_{i-1}, X) + \sum_l \mu_l g_l (y_i, X))) $$<br>因为Z与y无关，最大值与y无关。为了解决优化问题，我们可以使用最大积算法在CRF上，这样类似了Viteerbi算法在HMM上的应用。</p>
<h2 id="学习-1"><a href="#学习-1" class="headerlink" title="学习"></a>学习</h2><p>尽管整个图都是可观的，CRF的学习问题仍然是比较难于解决的。原因是学习中需要进行推断。给定训练集$\lbrace x_d, y_d\rbrace_{d=1}^N$，寻找到最优的$\lambda^{\ast}$和$\mu^{\ast}$。<br>\begin{equation}\begin{split} \lambda^{\ast}, \mu^{\ast} &amp;= argmax_{\lambda, \mu}\prod_{d=1}^N P(y_d\mid x_d, \lambda, \mu) \\<br>&amp;= argmax_{\lambda, \mu}\prod_{d=1}^N \frac{1}{Z(x_d, \lambda, \mu)}exp(\sum_{i=1}^n (\lambda^T f(y_{d, i} y_{d, i-1}, x_d) + \mu^T g(y_{d,i}, x_d))) \\<br>&amp;= argmax_{\lambda, \mu}\sum_{d=1}^T (\sum_{i=1}^n(\lambda^T f(y_{d,i}, y_{d, i-1}) + \mu^T g(y_{d, i}, x_d)) - log\ Z(x_d, \lambda, \mu)) \\<br>\end{split}\end{equation}<br>对$\lambda$求偏导：<br>$$ \Delta_{\lambda}L(\lambda, \mu) = \sum_{d=1}^N(\sum_{i=1}^n f(y_{d, i}, y_{d, i-1}, x_d) - \sum_y (P(y\mid x_d) \sum_{i=1}^n f(y_{d, i}, y_{d, i-1}, x_d))) $$<br>从上式中可以看出第一项是特征值，第二项是特征值的期望，另外对数判分函数以指数族的形式呈现时，其梯度是特征值的期望。<br>解决上面的式子需要处理指数级数量的数据求和，我们可以利用message passing算法来计算对势，这样得到一个闭环的形式。<br>\begin{equation}\begin{split} \sum_y (P(y\mid x_d)\sum_{i=1}^n f(y_i, y_{i-1}, x_d)) &amp;= \sum_{i=1}^n(\sum_y f(y_i, y_{i-1}, x_d) P(y\mid x_d)) \\<br>&amp;= \sum_{i=1}^n(\sum_{y_i, y_{i-1}} f(y_i, y_{i-1}, x_d) P(y_i, y_{i-1}\mid x_d)) \\<br>\end{split}\end{equation}<br>这样意味着，学习过程中包含有推断过程，通过message passing算法，学习过程只需要多项式时间久可以完成。<br>下面使用校准势来计算特征期望：<br>\begin{equation}\begin{split} \Delta_{\lambda}L(\lambda, \mu) &amp;= \sum_{d=1}^N(\sum_{i=1}^n f(y_{d, i}, y_{d, i-1}, x_d) - \sum_y (P(y\mid x_d) \sum_{i=1}^n f(y_{d, i}, y_{d, i-1}, x_d)))  \\<br>&amp;= \sum_{d=1}^N(\sum_{i=1}^n f(y_{d,i}, y_{d,i-1】， x_d} - \sum_{y_i, y_{i-1}}\alpha’(y_i, y_{i-1}) f(y_d, y_{d, i-1}, x_d))) \\<br>\end{split}\end{equation}<br>其中$\alpha’(y_i, y_{i-1}, x_d) = P(y_i, y_{i-1}\mid x_d)$。我们可以使用梯度上升法学习参数。<br>$$ \lambda^{(t+1)} = \lambda^{(t)} + \eta\Delta_{\lambda}L(\lambda^{(t)}, \mu^{(t)}) $$     $$ \mu^{(t+1)} = \mu^{(t)} + \eta\Delta_{\mu}L(\lambda^{(t)}, \mu^{(t)}) $$<br>在实际中，我们会加入正则项来提高参数的泛化能力。<br>$$ \lambda^{\ast}, \mu^{\ast} = argmax_{\lambda, \mu}\sum_{d=1}^N log\ P(y_d\mid x_d, \lambda, \mu) - \frac{1}{2\sigma^2} (\lambda^T \lambda + \mu^T\mu) $$<br>第二项叫做高斯先验，因为我们想让$\lambda^{\ast},\mu^{\ast}$趋近于0,这样可以减少特征值的数量。第二项也能叫做拉普拉斯先验，在条件概率中，我们不想看到零概率出现，因为零概率是病态的。梯度上升法收敛速度较慢，以使用共轭梯度法和拟牛顿法来加快速度。<br>从经验的表现来看，CRF比HMM和MEMM有所提升，特别是当非局部的影响明显时。虽然提升不够明显，但是CRF为一系列的问题的解决提供了很好的范例。CRF的另一优点是能够让使用者灵活的自己设计随机特征。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li>EM算法适应于处理存在隐变量的最大似然估计问题。</li>
<li>GMM和HMM被用于对静态和动态混合模型建模。</li>
<li>实现HMM主要需要处理的问题是，学习，推断和最大似然。推断可以通过前向和后向算法(变量去除)实现；最大似然问题可以通过Viterbi算法(最大积)实现；学习问题可以通过直接最大似然后者EM算法解决。</li>
<li>HMM具有十分强的马尔科夫性。HMM只能获得局部的关系，对于HMM的扩展MEMM，MEMM可以获得状态和全部可观序列之间的显性关系。但是，MEMM存在着标注偏置的问题。</li>
<li>CRF是部分有向的模型，其中转态之间是无向的，CRF使用全局的正规项克服了MEMM的标注偏置的问题。对于线性链式CRF，精确推断并不是困难的。推断问题可以通过最大积算法通过junction tree解决。学习问题可以通过梯度上升来解决最大似然。</li>
<li>具有任意图结构的CRF，精确推断就是比较困难的事情，这时就需要近似推断了，比如：采样，变分推断，loopy belief propagation。</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html" target="_blank" rel="noopener">http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html</a><br>[2] Wallach H M. Conditional random fields: An introduction[J]. Technical Reports (CIS), 2004: 22.<br>注：本文主要参考[1]中第12讲视频以及笔记。另外，本文中公式的和全部采用\sum，本文之前使用的都是\Sigma，后面也会使用\sum。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/25/EM算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/25/EM算法/" itemprop="url">EM算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-25T19:45:46+08:00">
                2018-05-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="混合高斯模型"><a href="#混合高斯模型" class="headerlink" title="混合高斯模型"></a>混合高斯模型</h1><p>$$ p(x_n\mid , \Sigma) = \Sigma_k \pi_k N(x\mid \mu, \Sigma_k) $$<br>其中$\pi_k$是混合参数，$N(x\mid \mu_k, \Sigma_k)$是其对应的高斯分布。<br>对于完全可观的独立同分布，对数似然可以分解为和的形式。<br>$$ l_c(\theta;D) = log p(x,z\mid \theta) = log p(z\mid \theta_z) + log p(z\mid z, \theta_x) $$<br>因为隐变量的存在，所有的变量会通过边缘概率耦合在一起。<br>因为对数里面有和的形式，解决有一定的困难，这样促使我们想到EM算法。</p>
<h1 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h1><p>给定数据集$(x_1, x_2, …, x_n)$，每个观测量都是d维的向量。k-means的目的是将n个观测量分成k个集合，在给定$z= \lbrace z_1, z_2, …, z_n \rbrace $。为了最小化组内平方和，我们随机的初始化类别向量，然后交替进行两步，知道收敛。</p>
<ul>
<li>E步：将每个观测分配到聚类中，是组内平方和最小。直观上来看就是讲数据点分配到最近的中心。<br>$$ z_i^{(t)} = argmin_k (x_i - \mu_k^{(t)})^T\Sigma_k^{-1^{(t)}}(x_i - u_k^{(t)}) $$</li>
<li>M步：重新计算中心值。<br>$$ \mu_k^{(t+1)} = \frac{\Sigma_i \delta(z_i^{(t)}, k)x_i} {\Sigma_i \delta(z_i^{(t)}, k)} $$<br>可以这么来理解EM，每个聚类可以看做具有相同的分布，比如$p(x_i\mid z_i = k)~N(x_i\mid \mu_k, \Sigma_kl) $我们希望可以学习到每个分布的参数$\mu_k$和$\Sigma$。</li>
</ul>
<h1 id="EM-算法"><a href="#EM-算法" class="headerlink" title="EM 算法"></a>EM 算法</h1><p>EM算法可以有效地迭代计算存在隐变量的最大似然估计。在最大似然估计值中，我们希望能够估计出对于每个观测数据最有可能的参数。<br>期望完全对数似然函数：<br>$$ \langle l_c(\theta;x,z)\rangle = \Sigma_n\langle logp(z_n\mid \pi)\rangle_{p(z\mid x)} + \frac{1}{2}\Sigma_n\Sigma_k\langle z_n^k\rangle((x_n - \mu_k)^T\Sigma_K^{-1}(x_n-\mu_k)+log|\sigma_k|+C) $$</p>
<p>EM算法是利用迭代地方式最大化$\langle l_c(\theta;x,z)\rangle$。在E步中，我们利用当前参数估计量计算隐变量的充分估计量。<br>$$ \tau_n^{k(t)} = \langle z_n^k\rangle_{q(t)} = p(zn^k = 1\mid x,\mu^{(t)},\Sigma^{(t)}) = \frac{\pi_k^{(t)}N(x_n\mid \mu_k^{(t)},\Sigma_k^{(t)})} {\Sigma_i \pi_i^{(t)}N(x_n\mid \mu_k^{(t)},\Sigma_k^{(t)})}$$<br>在M步中，使用期望值来计算参数期望的最大值。<br>\begin{equation}\begin{split} \pi_k &amp;= \Sigma_n \langle z_n^k \rangle_{q^{(t)}}/N = \Sigma_n \tau_n^{k(t)}/N = \langle n_k \rangle /N\\<br>\mu_k^{(t+1)} &amp;= \frac{\Sigma_n \tau_n^{k(t)}x_n}{\Sigma_n \tau_n^{k(t)}}\\<br>\Sigma_k^{(t+1)} &amp;= \frac{\Sigma_n \tau_n^{k(t)}(x_n-\mu_k^{(t+1)})(x_n-\mu_k^{(t+1)})^T}{\Sigma_n \tau_n^{(k(t))}}\\<br>\end{split}\end{equation}</p>
<h1 id="比较K-means和EM"><a href="#比较K-means和EM" class="headerlink" title="比较K-means和EM"></a>比较K-means和EM</h1><p>EM算法类似于K-means处理混合高斯模型，对于K-means，在E步中，我们制定每个聚类点，在M中我们假定每个点属于一个聚类重新计算聚类点。在EM算法中，我们使用概率的方式指定点为聚类点，在M步中，我们假定每个点数一个聚类以概率的重新计算聚类中心。</p>
<h1 id="EM算法理论依据"><a href="#EM算法理论依据" class="headerlink" title="EM算法理论依据"></a>EM算法理论依据</h1><p>X记作观测变量，Z记作隐变量集，分布模型为$p(x,z\mid \theta)$。<br>如果Z是可观的我们定义对数似然函数为：$ l_c(\theta;x,z) = log\ p(x,z\mid \theta) $。对于Z是可观的，我们最大化完全对数似然。<br>然而，当Z不可观时，我们必须最大化边际似然，也就是不完全对数似然函数。<br>$$ l_c(\theta;x) = log\ p(x\mid \theta) = log\Sigma_z p(x,z\theta) $$<br>我们必须将不完全对数似然解耦，因为对数里面具有和的形式。<br>为了解决这个问题，我们引入了平均分布$q(z\mid x)$来取代z的随机性。期望完全对数似然可以定义为：<br>$$ \langle l_c(\theta;x,z)\rangle_q = \Sigma_z q(z\mid x,\theta)log\ p(x,z\mid \theta) $$<br>根据杰西不等式：<br>\begin{equation}\begin{split} l(\theta;x) &amp;= log\ p(x\theta) \\<br>&amp;= log \Sigma_z p(x,z\mid \theta) \\<br>&amp;= log \Sigma_z q(z\mid x)\frac{p(x,z\mid \theta)}{q(z\mid x)} \\<br>&amp;\geqslant \Sigma_z q(z\mid x)log \frac{p(x,z\mid \theta)}{q(z\mid x)} \\<br>\end{split}\end{equation}<br>$$ l(\theta;x) \geqslant \langle l_c(\theta;x,z)\rangle_q + H_q $$<br>固定数据x，定义一个函数叫做自由能：<br>$$ F(q,\theta) = \Sigma_z q(z\mid x) log\frac{p(x,z\mid \theta)}{q(z\mid x)} \leq l(\theta;x)$$<br>这样EM算法等同于在F上进行坐标上升法：</p>
<ul>
<li>E步：$q^{t+1} = argmax_q F(q,\theta^t) $</li>
<li>M步：$\theta^{t+1} = argmax_{\theta} F(q^{t+1},\theta^t)$<br>$q^{t+1}(z\mid x)$是隐变量在给定数据和参数下的后验分布。$q^{t+1}=argmax_q F(q,\theta^t)=p(z\mid x,\theta^t) $<br>证明：这样的设置可以保证$l(\theta;x)\geqslant F(q,\theta)$<br>\begin{equation}\begin{split} F(p(z\mid x,\theta^t), \theta^t) &amp;= \Sigma_z q(z\mid x) log\frac{p(x,z\mid \theta)}{q(z\mid x)}\\<br>&amp;= \Sigma_z q(z\mid x) log\ p(x\mid \theta^t) \\<br>&amp;= log\ p(x\mid \theta^t) \\<br>&amp;= l(\theta^t;x) \\<br>\end{split}\end{equation}<br>同样可以用变分微分来表示：<br>$$ l(\theta;x) - F(q,\theta) = KL(q||p(z\mid x, \theta)) $$<br>在不失一般性的情况下，我们可以将$p(x,z\mid \theta)$定义为广义指数族分布：<br>$$ p(x,z\mid \theta) = \frac{1}{Z(\theta)} h(x,z) exp \lbrace \Sigma_i \theta_i f_i(x,z) \rbrace $$<br>如果$p(X\mid Z)$是广义线性模型，那么$f_i(x,z)=\eta_i^T(z)\xi_i(x) $。<br>在$q^{t+1}=p(z\mid x,\theta^t)$下，期望完全对数似然为：<br>\begin{equation}\begin{split} \langle l_c(\theta;x,z)\rangle_{q^{t+1}} &amp;= \Sigma_z q(z\mid x,\theta^t)log\ p(x,z\mid \theta^t) - A(\theta) \\<br>&amp;= \Sigma_i \theta_i^t \langle f_i(x,z)\rangle_{q(z\mid x, \theta^t)} - A(\theta) \\<br>&amp;= \Sigma_i \theta_i^t \langle \eta_i(z)\rangle_{q(z\mid x, \theta^t)}\eta_i(x) - A(\theta) \\<br>\end{split}\end{equation}<br>下面分析EM算法的M步，M步可以看做是最大化期望对数似然：<br>\begin{equation}\begin{split} F(q,\theta) &amp;= \Sigma_z q(z\mid x) log\frac{p(x,z\mid \theta)}{q(z\mid x)} \\<br>&amp;= \Sigma_z q(z\mid x)log\ p(x,z\mid \theta) - \Sigma_z q(z\mid x)log\ q(z\mid x) \\<br>&amp;= \langle l_c(\theta;x, z) \rangle_q + H_q<br>\end{split}\end{equation}<br>这样将自由能分解成两个部分，第一部分是期望完全对数似然，第二部分是熵，并且与变量$\theta$无关，这样最大自由能就等价于最大化期望完全对数似然。<br>$$ \theta^{t+1} = argmax_{\theta} \langle l_c(\theta;x,z)\rangle_{q^{t+1}} = argmax_{\theta} \Sigma_z q(z\mid x)log\ p(x,z\mid \theta) $$<br>在最优的$q^{t+1}$的情况下，这样就等同于解决标准的完全可观模型$p(x,z\mid \theta)$的最大似然问题，用$p(z\mid x, \theta)$取代包含z的充分统计量。</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>EM算法是一种对隐变量模型最大似然函数的一种方法，将比较难以解决的问题分解为两步：</p>
<ol>
<li>基于当前参数和可观数据对隐变量进行估计。</li>
<li>基于观测数据和隐变量对参数做极大似然估计。</li>
</ol>
<ul>
<li>EM算法好的方面<ul>
<li>没有学习率参数</li>
<li>自动限制参数</li>
<li>低维速度快</li>
<li>每代都可以确保调高似然</li>
</ul>
</li>
<li>不好的方面<ul>
<li>会陷入局部极优</li>
<li>比共轭梯度慢，特别是接近收敛时</li>
<li>需要代价高的推测过程</li>
<li>是一种最大似然或者最大后验的方法</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/18/论文总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/18/论文总结/" itemprop="url">论文总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-18T19:21:20+08:00">
                2018-05-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/17/可观无向图模型中的学习问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/17/可观无向图模型中的学习问题/" itemprop="url">可观无向图模型中的学习问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-17T10:58:53+08:00">
                2018-05-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="最大似然结构学习"><a href="#最大似然结构学习" class="headerlink" title="最大似然结构学习"></a>最大似然结构学习</h1><h2 id="连续型马尔科夫随机场"><a href="#连续型马尔科夫随机场" class="headerlink" title="连续型马尔科夫随机场"></a>连续型马尔科夫随机场</h2><p>给定高斯图模型，我们可以用一个伊辛模型来呈现。<br>$$p(x\mid \mu,\Sigma)=\frac{1}{(2\pi)^{k/2}|\Sigma|^{\frac{1}{2}}}exp \lbrace  -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \rbrace$$<br>下面令$\mu=0$和$Q=\Sigma_{-1}$，高斯模型可以写成：<br>$$p(x\mid \mu,Q)=\frac{|Q|^{1/2}} {(2\pi)^{k/2}}exp \lbrace  -\frac{1}{2}()\Sigma_i q_{ii} (x_i)^2 - \Sigma_{i&lt;j} q_{ij} x_i x_j) \rbrace$$<br>我们将上式指数中的第一部分看做是定义在节点上的势函数，第二部分是定义在比边上的势函数，也就是说等同于一个伊辛模型。<br>$$ P(x\mid \Theta) = exp\ (\Sigma_{i\in V} \theta_{ii}^t x_{d,j} + \Sigma_{(i,j)\in E} x_{d,i}x_{d,j} - A(\Theta)) $$</p>
<h2 id="稀疏图模型"><a href="#稀疏图模型" class="headerlink" title="稀疏图模型"></a>稀疏图模型</h2><p>协方差矩阵有一个重要的性质是：当$\Sigma_{i,j}=0$有$x_i\perp x_j$；逆协方差矩阵（精确矩阵）的对应的性质为：当$\Sigma_{i,j}^{-1}=0$时$x_i\perp x_j\mid x_{-ij}$。<br>如果出现$p \gg n$时，得不到最大似然估计，这是我们使用近邻选择得方法，增加惩罚函数来学习稀疏的图模型。<br>近邻选择可以看做是伊辛模型。<br>$$ P(x\mid \Theta) = exp\ (\Sigma_{i\in V} \theta_{ii}^t x_{d,j} + \Sigma_{(i,j)\in E} x_{d,i}x_{d,j} - A(\Theta)) $$</p>
<h1 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h1><h2 id="似然条件"><a href="#似然条件" class="headerlink" title="似然条件"></a>似然条件</h2><p>有向图中，对数似然可以分解为一组和的形式，每个对应于一个子节点对应其父节点。无向图中，对数似然并不能分解，因为Z是包含了所有的参数的函数。<br>$$ p(x) = \frac{1}{Z} \prod_{c\in C} \psi_c{x_c}, Z = \Sigma_x \prod_{c\in C} \psi_c(x_c) $$<br>我们需要通过推测来学习参数。我们获得了输入数据的充分统计量，计数。<br>$$total\ count:m(x) = \Sigma_n \delta(x, x_n)$$  $$cliqu\ count:m(x_c) = \Sigma_{x_{V\setminus c}} m(x)$$<br>似然函数为：<br>$$p(D\mid \theta) = \prod_n \prod_x p(x\mid \theta)^{\delta(x,x_n)}$$</p>
<p>\begin{equation}\begin{split} log\ p(D\mid \theta)&amp;=\Sigma_n \Sigma_x \delta(x,x_n)log\ p(x\mid \theta\\<br>l &amp; = \Sigma_x m(x)log(\frac{1}{Z}\prod_c \pi_c(x_c))\\<br>&amp; = \Sigma_c \Sigma_{x_c}m(x_c)log\ \psi_c(x_c) - N log\ Z \\<br>\end{split}\end{equation}<br>上式的两个部分对$\psi_c(x_c)$求导：<br>第一项：<br>$$ \frac{\partial l_1}{\partial \psi_c(x_c)} = m(x_c)/\psi_c(x_c) $$<br>第二项：<br>\begin{equation}\begin{split} \frac{\partial log\ Z}{\partial \psi_c(x_c)} &amp; = \frac{1}{Z} \frac{\partial}{\partial\psi_c(x_c)}(\Sigma_{\tilde{x} } \prod_{d} \psi_d(\tilde x_d))\\<br>&amp; = \frac{1}{Z} \Sigma_{\tilde{x}}\delta(\tilde x_c, x_c)\frac{\partial}{\partial \psi_c(x_c)}(\prod_{d} \psi_d(\tilde x_d) \\<br>&amp; =  \Sigma_{\tilde x}\delta(\tilde x_c, x_c) \frac{1}{\psi_c(\tilde x_c)} \frac{1}{Z} \prod_d \psi_d(\tilde x_d)\\<br>&amp; = \frac{1}{\psi_c (x_c)}\Sigma_{\tilde x} \delta(\tilde x_c, x_c) p(\tilde x)  \\<br>&amp; = \frac{x_c}{\psi_c (x_c)}<br>\end{split}\end{equation}</p>
<p>令导数为零，有：$\frac{\partial l}{\partial \psi_c(x_c)} = \frac{m(x_c)}{\psi_c(x_c)} - N\frac{p(x_c)}{\psi_c(x_c)} = 0$，从结果可以看出，模型的边缘概率密度等于观测的边缘概率密度。<br>$$ p_{MLE}^{\star} (x_c) = \frac{m(x_c)}{N} = \tilde p(x_c) $$<br>但是结果并没有给出似然参数的估计方法，只是给出了必须满足的条件。</p>
<h2 id="可分解模型"><a href="#可分解模型" class="headerlink" title="可分解模型"></a>可分解模型</h2><p>对于可分解的模型，势函数可以定义于最大团上，团势函数的最大似然等价于经验边际。因此最大似然可以通过检查得到。基于势函数的表示似然$p(x)=\frac{\prod_c \psi_c(x_c)}{\prod_s \psi_s(x_s)}$，其中c是最大团，s是最大团分离的因子。为了计算团势，将他们等同于经验边际。分离因子必须分解为几个邻居，那么$Z=1$。</p>
<h3 id="例一"><a href="#例一" class="headerlink" title="例一"></a>例一</h3><p>考虑链$X_1-X_2-X_3$，有团$(x_1, x_2),(x_2, x_3)$，分离因子$x_2$。<br>$$ \tilde p_{MLE}(x_1, x_2, x_3) = \frac{\tilde p(x_1, x_2)\tilde p(x_2, x_3)}{\tilde p(x_2)} $$<br>$$ \tilde{\psi}_{12}^{MLE}(x_1, x_2) = \tilde p(x_1, x_2) $$<br>$$ \tilde{\psi}_{23}^{MLE}(x_2, x_3) = \frac{\tilde p(x_2, x_3)}{\tilde p(x_2)}= \tilde p(x_2\mid x_3) $$</p>
<h3 id="例二"><a href="#例二" class="headerlink" title="例二"></a>例二</h3><p><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/%E6%97%A0%E5%90%91%E5%9B%BE%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1.png" alt=""><br>团$(x_1, x_2, x_3),(x_2, x_3, x_4)$，分离因子$x_2,x_3$<br>$$ \tilde p_{MLE}(x_1, x_2, x_3, x_4) = \frac{\tilde p(x_1, x_2, x_3)\tilde p(x_2, x_3, x_4)}{\tilde p(x_2, x_3)} $$<br>$$ \tilde{\psi}_{123}^{MLE}(x_1, x_2,x_3) = \frac{\tilde p(x_1, x_2, x_3)}{\tilde p(x_2, x_33)}= \tilde p(x_1\mid x_2, x_3) $$<br>$$ \tilde{\psi}_{234}^{MLE}(x_2, x_3, x_4) =  \tilde p(x_2, x_3, x_4) $$</p>
<h2 id="不可分离模型-IPF和GIS"><a href="#不可分离模型-IPF和GIS" class="headerlink" title="不可分离模型-IPF和GIS"></a>不可分离模型-IPF和GIS</h2><p>如果一个图是不可分离的，也就是说势函数不能以最大团定义，我们不能将经验边际等价于势函数的似然。</p>
<h3 id="Tabular-Cique-Potentials-IPF"><a href="#Tabular-Cique-Potentials-IPF" class="headerlink" title="Tabular Cique Potentials-IPF"></a>Tabular Cique Potentials-IPF</h3><p>如果团势函数是表格形式或者可以使用简介的参数模型表示，那么我们可以使用IPF来求最大似然估计。<br>从上面的似然函数导数：<br>$$\frac{\partial l}{\partial \psi_c(x_c)} = \frac{m(x_c)}{\psi_c(x_c)} - N\frac{p(x_c)}{\psi_c(x_c)} = 0$$<br>我们可以得到：<br>$$ \frac{\tilde p(x_c)}{\psi_c(x_c)} = \frac{p(x_c)}{\psi_c(x_c)} $$<br>因为$\psi_c$隐性的出现了模型边际$p(x_c)$中，所以在点$\psi_c$上存在等式关系，所以直接解出$\psi_c$是非常困难的，因为其出现在了等式两边。<br>我们将$\psi_c$在右边固定，然后在左边解出，循环所有的团，进行下面的迭代：<br>$$ \psi_c^{(t+1)}(x_c) = \psi_c^{(t)}(x_c)\frac{\tilde p(x_c)}{p^{(t)}(x_c)} $$<br>上面的算法可以看做是坐标上升算法，其中的坐标就是势团的参数。因此在每一步中，似然函数的值会增加，这样就可以收敛到全局极大值。这个算法童谣可以被看做是最小化观测数据的分布与模型分布的KL散度(交叉熵),只有当模型是可分解的。<br>$$ max\ l \Leftrightarrow KL(\tilde p(x)\mid \mid p(x\mid\theta))=\Sigma_x \tilde p(x)log\frac{\tilde p(x)}{p(x\mid \theta)} $$</p>
<p>\begin{equation}\begin{split} KL(q(x_a, x_b)\mid \mid p(x_a, x_b)) &amp;= \Sigma_{x_a, x_b}q(x_a)q(x_b\mid x_a)log\frac{q(x_a)q(x_b\mid x_a)}{p(x_a)p(x_b\mid x_a)}\\<br>&amp;= \Sigma_{x_a, x_b}q(x_a)q(x_b\mid x_a)log\frac{q(x_a)}{p(x_a)} + \Sigma_{x_a, x_b}q(x_a)q(x_b\mid x_a)log\frac{q(x_b\mid x_a)}{p(x_b\mid x_a)}\\<br>&amp;= KL(1(x_aq)\mid\mid p(x_a)) + \Sigma_x q(x_a)KL(q(x_b\mid x_a)\mid\mid p(x_b\mid x_a))\\<br>\end{split}\end{equation}<br>将上面两个式子合并：<br>$$ KL(\tilde p(x)\mid\mid p(x\theta)) = KL(\tilde p(x_c)\mid \mid p(x_c\theta)) + \Sigma_{x_a}\tilde p(x_c)KL(\tilde p(x_{-c})\mid\mid p(x_{-c}\mid x_c)) $$<br>从上面的式子可以看出，改变团势$\psi$并不会改变条件分布，所以对上式中第二项不会产生影响。<br>为了最小化第一项，和IPF的方法一样，我们将边际分布逼近观测分布。<br>可以将IPF解释为：保存原有的条件分布$p^{(t)}(x_{-c}\mid x_c)$，用观测数据分布取代原有的边际分布。</p>
<h3 id="Feature-based-Clique-Potentials"><a href="#Feature-based-Clique-Potentials" class="headerlink" title="Feature-based Clique Potentials"></a>Feature-based Clique Potentials</h3><p>当团势不能用表格表达时，或者更加一般的来说，获得团势是指数级的推测难度，必须要从有限的数据中学习指数个参数。我们可以将团变小，但是这样需要更多的独立性假设，并且改变了图模型结构。另外一种方法是不改变模型结构，使用更加一般的团势的参数。这被叫做基于特征的方法。<br>特征是对于一般的设定为空，对于少数特别的设定会有高低之分。每个特征函数都能变成小团势，然后将小团势乘起来就可以得到团势了。<br>例如，一个团势$\psi(x_1, x_2, x_3)$：<br>$$ \psi_c(x_a, x_2, x_3) = e^{\theta_{ing}f_{ing}}\times e^{\theta_{?ed}f_{?ed}} \times … = exp\lbrace \Sigma_{k=1}^T \theta_k f_k(c_1, c_2, c_3)\rbrace $$<br>每个特征都有一个权重$\theta_k$，它可以用增加或者减少团势的概率。<br>团上的边际分布就是一个广义指数族：<br>$$ p(c_1, c_2, c_3) \propto exp(\theta_{ing}f_{ing}(x_1, x_2, x_3) + \theta_{?ed}f_{?ed}(x_1, x_2, x_3) + …) $$<br>一般来说，特征可能是重叠的，没有限制的显示因子或者任意团变量的子集：<br>$$ \psi_c(x_c) = exp\lbrace \Sigma_{i\in I_c} \theta_k f_k(x_{c_i})\rbrace $$<br>我们可以将团势向前面一样乘起来：<br>$$ p(x) = \frac{1}{Z(\theta)}\psi_c (x_c) = \frac{1}{Z(\theta)}exp\lbrace \Sigma_c\Sigma_{i\in I_c} \theta_k f_k(x_{c_i})\rbrace $$<br>可以简化为：<br>$$ p(x) = \frac{1}{Z(\theta)}exp(\Sigma_i \theta_i f_i(x_{c_i})) $$<br>这就是指数族模型，特征值就是充分统计量。</p>
<h2 id="Gerneralized-Iterative-Scaling-GIS"><a href="#Gerneralized-Iterative-Scaling-GIS" class="headerlink" title="Gerneralized Iterative Scaling - GIS"></a>Gerneralized Iterative Scaling - GIS</h2><p>基于之前的概率函数，我们想通过一个算法求最大概率，然后有了GIS。<br>我们首先不是考虑直接优化目标，而是处理目标函数的下界。<br>\begin{equation}\begin{split} \tilde l(\theta;D) &amp;= l(\theta;D)/N \\<br>&amp;= \frac{1}{N}\Sigma_n log\ p(x_n\mid \theta) \\<br>&amp;= \Sigma_x \tilde p(x)log\ p(x\mid \theta) \\<br>&amp;= \Sigma_x \tilde p(x)\Sigma_i \theta_i f_i(x) - log\ Z(\theta) \\<br>&amp;\geqslant \Sigma_x \tilde p(x)\Sigma_i \theta_i f_i(x) - \frac{Z(\theta)}{Z(\theta^{(t)})} - log Z(\theta^{(t)}) + 1 \\<br>\end{split}\end{equation}</p>
<p>因为对数函数有一个线性上界：$log\ Z(\theta)\leqslant \mu Z(\theta) - log\ \mu - 1$<br>定义：$\Delta\theta_i^{(t)} = \theta_i-\theta_i^{(t)} $<br>\begin{equation}\begin{split}\tilde l(\theta;D) &amp;\geqslant \Sigma_x \tilde p(x)\Sigma_i \theta_i f_i(x) - \frac{Z(\theta)}{Z(\theta^{(t)})} - log Z(\theta^{(t)}) + 1 \\<br>&amp;= \Sigma_i \theta_i \Sigma_x \tilde p(x)f_i(x) - \frac{1}{Z(\theta^{(t)})}\Sigma_x exp(\Sigma_i \theta_i^{(t)} f_i(x))exp(\Sigma_i \Delta\theta_i^{(t)}f_i(x)) - log\ Z(\theta^{(t)}) + 1\\<br>&amp;= \Sigma_i \theta_i \Sigma_x \tilde p(x)f_i(x) - \Sigma_x p(x\mid \theta^{(x)})exp(\Sigma_i \Delta\theta_i^{(t)} f_i(x)) - log\ Z(\theta^{(t)}) + 1\\<br>\end{split}\end{equation}</p>
<p>假定$f_i(x) \geqslant,\Sigma_i f_i(x) = 1$，使用杰西不等式，$exp(\Sigma_i \pi_i x_i)\leqslant \Sigma_i \pi_i exp(x_i)$有：<br>$$ \tilde l(\theta;D) \geqslant \Sigma_i \theta_i \Sigma_x \tilde p(x)f_i(x) - \Sigma_x p(x\mid \theta^{(x)})\Sigma_i f_i(x)exp(\Delta\theta_i^{(t)}) - log\ Z(\theta^{(t)}) + 1 $$<br>令上面式子的右边等于$\Delta(\theta)$。<br>求偏导令其为零：<br>$$ \frac{\partial \Lambda}{\partial \theta_i} = \Sigma_x \tilde p(x)f_i(x) - exp(\Delta\theta_i^{(t)})\Sigma_x p(x\mid \theta^{(t)})f_i(x) = 0$$<br>$$ e^{\Delta\theta_i^{(t)}} = \frac{\Sigma_x \tilde p(x)f_i)(x)}{\Sigma_x p(x\mid \theta^{(t)})f_i(x)}= \frac{\Sigma_x \tilde p(x)f_i)(x)}{\Sigma_x p^{(t)}(x)f_i(x)}Z(\theta^{(t)}) $$<br>更新准则：<br>$$ \theta_i^{(t+1)} = \theta_i^{(t)} + \Delta\theta_i^{(t)} \Rightarrow p^{(t+1)}(x)=p^{(t)}(x)\prod_i e^{\Delta\theta_i^{(t)}} $$<br>\begin{equation}\begin{split} p^{(t+1)}(x) &amp;= \frac{p^{(t)}(x)}{Z(\theta^{(t)})}\prod_i (\frac{\Sigma_x \tilde p(x)f_i)(x)}{\Sigma_x p^{(t)}(x)f_i(x)}Z(\theta^{(t)}))^{f_i(x)} \\<br>&amp;= \frac{p^{(t)}(x)}{Z(\theta^{(t)})}\prod_i (\frac{\Sigma_x \tilde p(x)f_i)(x)}{\Sigma_x p^{(t)}(x)f_i(x)})^{f_i(x)}(Z(\theta^{(t)}))^{\Sigma_i f_i(x)} \\<br>&amp;= p^{(t)}(x)\prod_i (\frac{\Sigma_x \tilde p(x)f_i)(x)}{\Sigma_x p^{(t)}(x)f_i(x)})^{f_i(x)} \\<br>\end{split}\end{equation}</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="IPF"><a href="#IPF" class="headerlink" title="IPF"></a>IPF</h2><p>IPF是无向图模型求最大似然的一般方法。</p>
<ul>
<li>对$\psi_c$在每个团上一个固定的等式，坐标上升。</li>
<li>在最大团边际空间上进行I-projection</li>
<li>需要势函数完全的参数化</li>
<li>必须要以最大团描述</li>
<li>对于完全可分的模型，变成一次算法</li>
</ul>
<h2 id="GIS"><a href="#GIS" class="headerlink" title="GIS"></a>GIS</h2><ul>
<li>要以基于特征的势函数描述</li>
<li>GIS是IPF的特殊形式，GIS的团势是建立在特征值配置上的。</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html" target="_blank" rel="noopener">http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html</a><br>注：本文主要参考[1]中第8讲视频以及笔记。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/12/可观贝叶斯网络中的学习问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/12/可观贝叶斯网络中的学习问题/" itemprop="url">可观贝叶斯网络中的学习问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-12T15:37:27+08:00">
                2018-05-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="完全可观的图模型学习"><a href="#完全可观的图模型学习" class="headerlink" title="完全可观的图模型学习"></a>完全可观的图模型学习</h1><p>图模型学习的目的是在给定独立的样本集的情况下找到合适的的贝叶斯网络，这里的学习（learning）表示对参数的估计或者是从数据学习网络的拓扑结构。</p>
<h2 id="最大似然在信息论上的解释"><a href="#最大似然在信息论上的解释" class="headerlink" title="最大似然在信息论上的解释"></a>最大似然在信息论上的解释</h2><p>可以这样理解，将对数似然函数在数据上的和，转变为在变量状态上的和。<br>\begin{equation}\begin{split} l(\theta_G,G;D)&amp;=log\ p(D\mid \theta_G,G) (Joint Likelilood)\\<br>&amp; = log\ p\prod_n(\prod_i p(x_{n,i}\mid x_{n,\pi_{i(G)}}, \theta_{i\mid \pi_{i(G)}})  (BN Factorization  Rule)\\<br>&amp; = \Sigma_i(\Sigma_n log\ p(x_{n,i}\mid x_{n,\pi_{i(G)}}, \theta_{i\mid \pi_{i(G)}}))\\<br>&amp; = M\ \Sigma_i(\Sigma_{x_i,x_{\pi_{i(G)}}} \frac{count(x_i,x_{\pi_{i(G)}})}{M} log\ p(x_i\mid x_{\pi_{i(G)}}, \theta_{i\mid \pi(G)}))  \\<br>&amp; = M\ \Sigma_i(\Sigma_{x_i,x_{\pi_{i(G)}}} \hat{p}(x_i, x_{\pi_{i(G)}}) log\ p(x_i\mid x_{\pi_{i(G)}}, \theta_{i\mid \pi(G)}))  \\<br>\end{split}\end{equation}</p>
<p>这里的H表示变量状态，概率分布$p(x_i)$用计数函数取代（count function）。$(x_i,x_{\pi{i(G)}}$包括了所随机变量的值。<br>继续对上面的式子进行推导：<br>\begin{equation}\begin{split} l(\theta_G,G;D)&amp;=M\ \Sigma_i(\Sigma_{x_i,x_{\pi_{i(G)}}} \hat{p}(x_i, x_{\pi_{i(G)}}) log\ p(x_i\mid x_{\pi_{i(G)}}, \theta_{i\mid \pi(G)}))  \\<br>&amp; = M\ \Sigma_i(\Sigma_{x_i,x_{\pi_{i(G)}}} \hat{p}(x_i, x_{\pi_{i(G)}}) log\ \frac{p(x_i, x_{\pi_{i(G)}}\mid \theta_{i\mid \pi_{(G)}})}  {\hat{p}(x_i, x_{\pi_{i(G)}})}\frac{\hat{p}(x_i)}{\hat{p}(x_i)})\\<br>&amp; = M\ \Sigma_i(\Sigma_{x_i,x_{\pi_{i(G)}}} \hat{p}(x_i, x_{\pi_{i(G)}}) log\ \frac{p(x_i, x_{\pi_{i(G)}}, \theta_{i\mid \pi_{(G)}})} {\hat{p}(x_i, x_{\pi_{i(G)}})\hat{p}(x_i)}) - M\ \Sigma_i\Sigma_{x_i} - \hat{p}(x_i)log\ \hat{p}(x_i)\\<br>&amp; = M\ \Sigma_i\hat{I}(x_i,x_{\pi_{i(G)}}) - M\ \Sigma_i\hat{H}(x_i) \\<br>\end{split}\end{equation}<br>这样可以将最大似然估计分成两个部分，第一个部分是所有节点的互信息，第二部分是每个节点的熵。可以得出这样的结论，如果我们确定结构式树结构（每个节点只有一个父节点），那么基于最大似然估计下，我们可以获得一个最优的树。</p>
<h1 id="Chow-Liu-算法"><a href="#Chow-Liu-算法" class="headerlink" title="Chow-Liu 算法"></a>Chow-Liu 算法</h1><p>目标函数可以写成：<br>$$ l(\theta_G,G;D) = M\ \Sigma_i\hat{I}(x_i,x_{\pi_{i(G)}}) - M\ \Sigma_i\hat{H}(x_i)  $$<br>将上式后面各个变量的熵去掉，因为变量的熵与树的结构无关，上式化简为：<br>\begin{equation}\begin{split} C(G) &amp;= M\ \Sigma_i\hat{I}(x_i,x_{\pi_{i(G)}}) \\<br>&amp;= M\ \Sigma_i\hat{I}(x_i,x_j) \\<br>\end{split}\end{equation}<br>我们只需要计算经验分布（empirical distribution）和每对节点的互信息（mutual information）就可以了。经验分布可以通过数据直接数出来，下面是计算每对节点$x_i$和$x_j$之间的经验分布和互信息。<br>$$ \hat{p}(X_i, X_j) = \frac{count(x_i, x_j)}{M} $$<br>$$ \hat{I}(X_i, X_j) = \Sigma_{x_i, x_j}\hat{p}(x_i, x_j)log\ \frac{\hat{p}(x_i, x_j)}{\hat{p}(x_i)\hat{p}(x_j)} $$<br>我们定义一个有节点$x_1, x_2, x_3, …,x_n$的图，指定图的边$(i, j)$的权值为$\hat{I}(X_i, X_j)$。Chow-Liu算法可以计算最大权重生成树。挑选任意节点作为根节点，然后使用宽度优先算法（breadth-first-search）来决定方向。</p>
<h1 id="对于完全可观的给定结构的参数学习"><a href="#对于完全可观的给定结构的参数学习" class="headerlink" title="对于完全可观的给定结构的参数学习"></a>对于完全可观的给定结构的参数学习</h1><p>假定图结构固定，从N个独立同分布的样本集中进行参数估计$D = \lbrace x_1, x_2, x_3, …, x_N\rbrace$。一般来说，每个训练样本$x_n = x_{n, 1}, x_{n,2}, …, x_{n,M}$是M维的向量，对应于每个节点。下面介绍几个常见用于参数估计的分布。</p>
<h2 id="多项式模型"><a href="#多项式模型" class="headerlink" title="多项式模型"></a>多项式模型</h2><p>对于N个独立同分布的样本，采用unit basis vectors表示，$x_n = (x_{n,1}, x_{n,2}, …, x_{n,K})$，其中$x_{n,k}=\lbrace 0, 1 \rbrace $， $\Sigma_{k=1}^K x_{n, k} $。这种表示方法将事件抽离出来，不考虑事件本身的意义，关注事件发生与否。数据集$D = \lbrace x_1, x_2, x_3, …, x_N\rbrace$的似然函数为：<br>$$ L(\theta\mid D) = P(x_1, x_2, …, x_N\mid \theta) = \prod_{n=1}^N P(x_n\mid \theta) = \prod_k \theta_k^{n_k} $$<br>$$ l(\theta\mid D) = log\ \prod_k \theta_k^{n_k}  = \Sigma_k n_k log\ \theta_k $$<br>因为存在着等式约束$\Sigma_{k=1}^K x_{n,k} = 1$，所以需要在$l(\theta\mid D)$中加入Lagarange乘子。<br>$$ \hat{l}(\theta\mid D) = \Sigma_k n_k log\ \theta_k + \lambda(1 - \Sigma_{k=1}^K x_{n,k}) $$<br>对$\theta_k$求偏导并令其为零：<br>$$ \hat{\theta}_{k,MLE} = \frac{n_k}{N} $$<br>或者$$ \hat{\theta}_{k,MLE} = \frac{1}{N}\Sigma_n x_{n,k}$$<br>此外，$\bar{n} = {n_1, n_2, …, n_K}$和$n_k = \Sigma_n x_{n,k}$是数据集D的充分统计量。</p>
<h2 id="贝叶斯参数估计"><a href="#贝叶斯参数估计" class="headerlink" title="贝叶斯参数估计"></a>贝叶斯参数估计</h2><p>贝叶斯参数估计就是通过贝叶斯定理，利用先验概率分布来推测出后验概率分布，所以先验概率分布对于贝叶斯参数估计方法来说非常的重要，下面介绍两种常见的先验。</p>
<h3 id="狄利克雷先验-Dirichlet-Prior"><a href="#狄利克雷先验-Dirichlet-Prior" class="headerlink" title="狄利克雷先验(Dirichlet Prior)"></a>狄利克雷先验(Dirichlet Prior)</h3><p>Dirichlet Prior由一组超参数$\alpha_1, \alpha_2, …,\alpha_N$来定义。Dirichlet分布如下：<br>$$ P(\theta) = \frac{\Gamma(\Sigma_k \alpha_k)}{\prod_k \Gamma(\Sigma_k \alpha_k)} \prod_k \theta_k^{\alpha_k-1} = C(\alpha)\prod_k \theta_k^{\alpha_k-1} $$<br>其中$C(\alpha)$是正则化参数，后验概率可以写成如下形式：<br>$$ P(\theta\mid x_1, x_2, …, x_N) = \frac{P(x_1, x_2,…, x_N\mid \theta)P(\theta)}{P(x_1, x_2, …, x_N)} \propto \prod_k \theta_k^{\alpha_k + n_k -1} $$<br>因为后验概率与先验概率形式相同，所以被叫做共轭先验。也就是说，只要先验是Dirichlet分布，那么后验就必定是Dirichlet分布。<br>基于这一特性，就有了序列贝叶斯更新算法。由Dirichlet先验分布$P(\vec{\theta}\mid \vec{\alpha}) = Dir(\vec{\theta}\mid \vec{\alpha})$，后验更新为$P(\vec{\theta}\mid \vec{\alpha},\vec{n’}) = Dir(\vec{\theta}\mid \vec{\alpha}, \vec{n’})$，之后通过$N’$个样本，可以获得充分统计量$\vec{n’}$，后验变成：<br>$$ P(\vec{\theta}\mid \vec{\alpha},\vec{n’}, \vec{n’’}) = Dir(\vec{\theta}\mid \vec{\alpha}, \vec{n’}, \vec{n’’}) $$<br>观测另外$N’’$数据有充分统计量$\vec{n’’}$。这样序列化的处理数据方式和批处理是等价的。Dirichlet主要的缺点是一维的分布，不能处理多维的分布，对于多维有对数正态先验。</p>
<h3 id="对数正态先验"><a href="#对数正态先验" class="headerlink" title="对数正态先验"></a>对数正态先验</h3><p>对数正态先验相比于Dirichlet拥有更加丰富的分布性质。下面是对数先验的定义：<br>$$ \theta \sim LN_K(\mu, \Sigma) $$ $$ \gamma \sim N_{K-1}(\mu, \Sigma)\ \ \  \gamma_K = 0 $$  $$\theta_i \sim \lbrace \gamma_i - log(1 + \Sigma_{i=1}^{K-1} e^{\gamma_i}) $$<br>对数配分函数 $ C(\gamma) = log(1 + \Sigma_{i=1}^{K-1} e^{\gamma_i}) $<br>对数正态先验可以获得更好的协方差结构的性质，但是它不是共轭先验。</p>
<h3 id="多元正态分布的参数估计"><a href="#多元正态分布的参数估计" class="headerlink" title="多元正态分布的参数估计"></a>多元正态分布的参数估计</h3><p>高斯分布的概率密度函数为：<br>$$p(X; \mu,\Sigma)=\frac{1}{(2\pi)^{n/2}\Sigma^{\frac{1}{2}}}exp \lbrace  -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \rbrace$$<br>可以对$\mu$和$\Sigma$进行最大似然估计：<br>$$ \mu_{MLE} = \frac{1}{N}\Sigma_n x_n $$<br>$$ \Sigma_{MLE} = \frac{1}{N}\Sigma_n (x_n - \mu_{MLE})(x_n - \mu_{MLE})^T $$<br>我们需要主要到当$\Sigma$不是满秩的时候，其也不是可逆的。贝叶斯估计的优势在于贝叶斯估计具有先验知识，或者是先验是共轭的，这样就可以进行序列化的处理，类似于批处理的方式。<br>特别的，当$\mu$未知，$\sigma$已知时：<br>$$ p(\mu) = (2\pi\tau^2)^{-1/2} exp\lbrace -(\mu - \mu_0)^2 /2\tau^2 \rbrace $$<br>联合概率分布为：<br>$$ P(x,\mu) = (2\pi\tau^2)^{-1/2} exp\lbrace -(\mu - \mu_0)^2 /2\tau^2 \rbrace * (2\pi\sigma^2)^{-N/2} exp\lbrace -\frac{1}{2\sigma^2}\Sigma_{n=1}^N (x_n - \mu)^2 \rbrace $$<br>后验概率即为：<br>$$ P(\mu\mid X) = (2\pi\tilde{\sigma}^2)^{-N/2} exp\lbrace -\frac{1}{2\tilde{\sigma}^2} (\mu - \tilde{\mu})^2 \rbrace $$<br>其中$\mu = \frac{N/\sigma^2}{N/\sigma^2 + 1/\tau} \bar{x} + \frac{1/\tau}{N/\sigma^2 + 1/\tau}\mu_0 $和$ \tilde{\sigma}^2 = (\frac{N}{\sigma^2} + \frac{1}{\tau^2})^{-1} $<br>后验均值是先验和极大似然估计的凸组合，权值与噪声水平成正比。<br>后验$1/\tilde{\sigma}^2$是先验$1/\tau^2$与每个观测数据对于$1/\sigma^2$的影响。</p>
<h2 id="最大似然估计用于一般的贝叶斯网络"><a href="#最大似然估计用于一般的贝叶斯网络" class="headerlink" title="最大似然估计用于一般的贝叶斯网络"></a>最大似然估计用于一般的贝叶斯网络</h2><p>如果我们假定每个条件概率密度参数是全局独立的，所有的节点是可观的，那么对数似然函数可以分解为如下的形式：<br>$$ l(\theta; D) = log\ p(D\mid \theta) = \Sigma_i(\Sigma_n log\ p(x_{n,i} \mid x_{n,\pi_i}, \theta_i)) $$<br>对于独立同分布的数据似然函数为：<br>$$ p(D\mid \theta) = \prod_n p(x_n\mid \theta) $$</p>
<h3 id="最大似然估计用于离散形式的贝叶斯网络"><a href="#最大似然估计用于离散形式的贝叶斯网络" class="headerlink" title="最大似然估计用于离散形式的贝叶斯网络"></a>最大似然估计用于离散形式的贝叶斯网络</h3><p>假定每个条件概率分布都可以用一个表格来表示，其中$\theta_{ijk} = P(X_i = j\mid x_{\pi_i} = k)$，然后充分统计量就是所有可能的状态的和$ n_{ijk} = \Sigma_n x_{n,i}^j x_{n,{\pi_i}}^k $，对数似然函数写成：<br>$$ l(\theta; \mid D) = log\prod_{i,j,k}\theta_{ijk}^{n_{ijk}} = \Sigma_{i,j,k}n_{i,j,k} log\ \theta_{i,j,k} $$<br>其中$\Sigma_j \theta_{ijk} = 1$，使用拉格朗日乘数法可以得出结果：<br>$$ \theta_{ijk}^{ML} = \frac{n_{ijk}}{\Sigma_{j’} n_{ij’k}} $$</p>
<h2 id="贝叶斯参数估计-1"><a href="#贝叶斯参数估计-1" class="headerlink" title="贝叶斯参数估计"></a>贝叶斯参数估计</h2><ul>
<li>全局独立性 $p(\theta_m\mid G) = \prod_{i=1}^M p(\theta_i \mid G)$</li>
<li><p>局部独立性 $p(\theta_i\mid G) = \prod_{j=1}^{q_i} p(\theta_{x_i^k\mid x_{\pi_i^j}} \mid G)$<br>全局参数独立性指的是每个节点间的参数是独立的，局部参数独立性指的是节点的参数在其父节点不同的情况下独立。</p>
</li>
<li><p>离散的有向无环图模型满足$x_i\mid x_{\pi_i}^j \sim Multi(\theta)$，同时Dirichlet先验为$p(\theta) = C(\alpha)\prod_k \theta_k^{\alpha_k - 1}$。</p>
</li>
<li>高斯有向无环图模型满足$x_i\mid x_{\pi_i}^j \sim Normal(\mu,\Sigma)$，正态Wishart先验为：<br>$$ p(\mu\mid v,\alpha_{\mu},W) = Normal(v,(\alpha_{\mu}W)^{-1}) $$<br>$$ p(W\mid \alpha_w,T) = c(n, \alpha_w)|T|^{\alpha_w /2}|W|^{(\alpha_w -n-1)/2} exp \lbrace\frac{1}{2}tr\lbrace TW \rbrace\rbrace $$<br>其中$W = \Sigma^{-1}$。</li>
</ul>
<p>##　马尔科夫链转移矩阵<br>考虑一个时不变的一阶马尔科夫链，初始状态概率向量为$\pi_k = P(X_1^K = 1)$，状态转移矩阵$A_{ij} = P(X_t^j = 1\mid x_{t-1}^i = 1)$。联合概率为：<br>$$ P(X_{1:T\mid \theta}) = P(x_1\mid \pi)\prod_{t=2}^T P(X_t\mid X_{t-1})$$<br>对数似然函数为：<br>$$ l(\theta;D) = \Sigma_n log\ p(x_{n,1}\mid \pi) + \Sigma_n\Sigma_{t=2}^T log\ P(x_{n,t}\mid x_{n,t-1,A}) $$<br>A是随机矩阵并且$\Sigma_j A_{ij}$，所以$A_{ij}$的最大似然估计是从$i$到$j$转移的分式：<br>$$ A_{ij}^{ML} = \frac{\Psi(i\rightarrow j)}{\Psi(i\rightarrow \star)} = \frac{\Sigma_n\Sigma_{t=2}^T x_{n,t-1}^i x_{n,t}^j}{\Sigma_n\Sigma_{t=2}^T x_{n,t-1}^i} $$</p>
<p>上面的方法有一个稀疏的问题，当$i\rightarrow j$没有出现时，$A_{ij}=0$，那么即将出现的单词对$i\rightarrow j$概率为零。可以使用下面的方法进行解决：<br>$$ \tilde{A}_{i\rightarrow \star} = \lambda\eta_t + (1 - \lambda) A_{i\rightarrow \star}^{ML} $$</p>
<h2 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a>隐马尔科夫模型</h2><ul>
<li>两个状态之间转移的可能性$P(y_t^j = 1\mid y_{t-1}^i = 1) = a_{i,j}$或者$P(y_t\mid y_{t-1} = 1)\sim Multinomial(a_{i,1}, a_{i,2}, …, a_{i,M})$。</li>
<li>开始概率$P(y_1) \sim Multinomial(\pi_1, \pi_2,…, \pi_M)$。</li>
<li>每个y向x的传播概率$P(x_t\mid y_t^i = 1)\sim Multinomial(b_{i,1}, b_{i,2}, …, b_{i,K})$。</li>
</ul>
<p>给定$x=x_1,…,x_N$对实际的状态路径已知，定义如下：<br>$A_{ij} = \Psi$状态转移在y上从$i\rightarrow j$<br>$B_{ik} = \Psi $状态i在y中影响在x中的k<br>$\theta$的最大似然估计：<br>$$a_{ij}^{ML} = \frac{\Psi(i\rightarrow j)} {\Psi(i\rightarrow \star)} = \frac{A_{ij}}{\Sigma_j A_{ij}}$$<br>$$ b_{ik}^{ML} = \frac{\Psi(i\rightarrow \star)}{\Psi(i\rightarrow \star)} = \frac{B_{ij}}{\Sigma_k B_{ik}} $$</p>
<p>对于样本较小的情况下，采用伪计数。<br>$A_{ij} = \Psi$状态转移在$y+R_{ij$}$上从$i\rightarrow j$<br>$B_{ik} = \Psi$状态i在$y$中影响在$x+S_{ik}$中的k<br>$R_{ij}$，$S_{ij}$是伪计数，体现了我们对先验信息的信任。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>对于完全可观的贝叶斯网络，可以进行分解，所以学习问题可以进行分解。</p>
<ul>
<li>结构学习<ul>
<li>Chow Liu 算法</li>
<li>近邻选择</li>
</ul>
</li>
<li>在概率图的单个节点上进行学习-密度估计：指数族分布<ul>
<li>一般的离散分布</li>
<li>一般的连续分布</li>
<li>共轭先验</li>
</ul>
</li>
<li>两个节点进行学习：广义线性模型<ul>
<li>条件概率密度估计</li>
<li>分类</li>
</ul>
</li>
<li>更多的节点<ul>
<li>利用局部的性质</li>
</ul>
</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html" target="_blank" rel="noopener">http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html</a><br>注：本文主要参考[1]中第7讲视频以及笔记。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/09/指数族与广义线性模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/09/指数族与广义线性模型/" itemprop="url">指数族与广义线性模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-09T10:30:15+08:00">
                2018-05-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="指数族"><a href="#指数族" class="headerlink" title="指数族"></a>指数族</h1><p>将随机变量X写成指数族的形式：<br>$$p(X=x;\eta)=h(x)exp(\eta^T T(x)-A(\eta))$$<br>其中：$\eta$是自然参数向量（natural paramater），T(x)是充分统计量（sufficient statistic），$A(\eta)$是对数判分函数（log partition function）。</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>指数族可以包括许多的例子，比如高斯分布，伯努利分布，多项式分布等。</p>
<h3 id="多元正态分布"><a href="#多元正态分布" class="headerlink" title="多元正态分布"></a>多元正态分布</h3><p>令向量$X\in R^k$<br>$$p(x\mid \mu,\Sigma)=\frac{1}{(2\pi)^{k/2}|\Sigma|^{\frac{1}{2}}}exp \lbrace  -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \rbrace$$<br>$$=\frac{1}{(2\pi)^{k/2}} exp\lbrace -\frac{1}{2} tr(\Sigma^{-1}x x^T) + \mu^T \Sigma^{-1}x^T- \frac{1}{2}\mu^T \Sigma^{-1}\mu - log|\Sigma|\rbrace$$<br>对应的指数族表示：<br>$$\eta = [\Sigma^{-1}\mu; -\frac{1}{2}vec(\Sigma^{-1})]$$  $$ T(x)=[x;vec(xx^T)]$$  $$ A(\eta)=\frac{1}{2} \mu^T \Sigma^{-1} \mu + log|\Sigma| $$  $$ h(x)= \frac{1} { {2\pi}^{k/2} } $$</p>
<h3 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h3><p>$$ p(x;\phi) $$  $$ = \phi^x(1-\phi)^{1-x} $$  $$ = exp(log(\phi^x(1-\phi)^{1-x}) $$  $$ = exp(log(\phi^x)+log((1-\phi)^{1-x})) $$  $$ = exp(xlog(\phi) + (1-x)log(1-\phi)) $$  $$ = exp(xlog(\frac{\phi}{1-\phi})+log(1-\phi))$$<br>对应于指数族：<br>$$ \eta = log(\frac{\phi}{1-\phi}) $$  $$ T(x) = x $$  $$ A(\eta) = -log(1-\phi) $$  $$ h(x) = 1 $$</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>很多的分布可以看做是指数族：单变量高斯分布（the univariate Gaussian)，泊松分布（Poisson）， 多项分布（multinomial），线性回归（linear regression），伊辛模型（Ising model），受限波尔兹曼机机（restricted Boltzmann machines），还有条件随机场（contional random field，CRFs）。</p>
<h4 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h4><p><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/CRF.png" alt=""><br>条件随机场是基于上面图的无向图模型，势函数是定义在成对输出上面的。<br>$$ p_\theta(y\mid x)=\frac{1}{Z(x)}exp(\Sigma_{e\in E,k} \lambda_k f_k(e,y\mid_e, x) + \Sigma_{v\in V,k} \mu_k g_k(v,y\mid_v, x))$$<br>其中$f_k$和$g_k$是固定的，$g_k$是波尔顶点特征，$f_k$是波尔边特征。</p>
<h2 id="指数族特性"><a href="#指数族特性" class="headerlink" title="指数族特性"></a>指数族特性</h2><p>指数族具有如下的特性：</p>
<ol>
<li>对数配分函数的第d阶导数，是充分统计量的第d阶中心距。<br>比如：对数配分函数的一阶导数是T(X)的均值，其二阶导是T(X)的方差。</li>
<li>因为对数配分函数的二阶导是正的，所以对数配分函数是凸的，因此方差总是非负的。</li>
<li>我们可以将对数配分函数的一阶导看成自然参数的函数，然后令其为零，反过来利用距参数就可以解决自然参数，记作：$\eta = \psi(\mu)$ 。</li>
<li>在指数族上进行最大似然估计与矩匹配是一致的。<ul>
<li>写出一般指数族的对数似然函数:<br>$$ const + \eta^T (\Sigma_{i=1}^n T(x_i)) - nA(\eta) $$</li>
<li>求似然函数的梯度：<br>$$ \Sigma_{i=1}^n T(x_i)) - n\Delta_\eta A(\eta) $$</li>
<li>令$\Delta_\eta A$为零：<br>$$ \Delta_\eta A = \frac{1}{n}\Sigma_{i=1}^T T(x_i) \Rightarrow \mu = \frac{1}{n}\Sigma_{i=1}^T T(x_i) \Rightarrow 矩估计=样本距 $$</li>
</ul>
</li>
</ol>
<h3 id="充分统计量"><a href="#充分统计量" class="headerlink" title="充分统计量"></a>充分统计量</h3><p><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE.png" alt=""><br>从贝叶斯的观点出发：如果T具备了我们预测参数$\theta$的所有信息（即T是充分统计量），那么$\theta \perp X \mid T \Rightarrow P(\theta \mid X, T)=P(\theta\mid T)$。<br>从频率学派的角度出发：如果T已知的用来产生数据的参数，那么$ X \perp \theta \mid T \Rightarrow P(X\mid T;\theta) = P(X\mid T) $<br>从马尔科夫随机场的角度进行考虑：<br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/%E5%85%85%E5%88%86%E7%BB%9F%E8%AE%A1%E9%87%8F.png" alt=""></p>
<h2 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h2><p>重新从贝叶斯的角度出发，写出给定自然参数的似然函数，我们选择了一个自然参数先验，然后计算出自然参数的后验概率。<br>比如：<br>$$ p(x\mid \eta) \propto exp(\eta^ T T(x) - A(\eta))$$<br>$$ p(\eta) \propto exp(\xi^T T(\eta) - A(\xi)) $$<br>$$ p(\eta\mid x, \xi) \propto exp(\eta^T T(x) + \xi^T T(\eta) + A(\eta) + A(\xi)) $$<br>如果$\eta = T(\eta)$ ，那么后验概率变为：<br>$$ p(\eta\mid x, \xi) \propto exp(T(\eta)(T(x) + \xi)+ A(\eta) + A(\xi)) $$<br>当$ \eta = T(\eta)$ ，我们指定$\eta ~exponentialFamily$，这是先验就是共轭先验。</p>
<h1 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h1><p>广义线性模型可以将分类和回归问题进行统一，使用相同的统计框架。<br>假定：<br>$$ Y \sim exponentialFamily $$<br>$$ \eta = \psi(\mu=f(\xi = \theta^T x)) $$<br>其中Y是响应，x是固定输入，$\theta$是需要学习的参数，$f$（响应函数，response function），$\psi$增加了一定的灵活性，f经常被设定为$\psi^{-1}$（canonical response function）。</p>
<h1 id="广义线性模型的批学习"><a href="#广义线性模型的批学习" class="headerlink" title="广义线性模型的批学习"></a>广义线性模型的批学习</h1><p>考虑通过求导的方法来解决最小二乘问题，就是使代价函数达到极小：<br>$$ J(\theta) = \frac{1}{2} \Sigma_{i=1}^n (x_i^T\theta - y_i)^2 = \frac{1}{2} (X\theta-y) $$<br>$x_i$表示第i个输入样本，$y_i$表示第i个输出样本。<br>对$J(\theta)$求一阶导并令其为零，可以得到取得极小值时的$\theta$。<br>$$ \triangledown J(\theta) = X^T X\theta - X^T y = 0 \Rightarrow \theta^* = (X^T X)^{-1} X^T y $$<br>使用牛顿法进行迭代寻找最优解，牛顿法更新参数更新准则：<br>$$ \theta^{t+1} = \theta^t - H^{-1}\triangledown J(\theta) $$</p>
<p>对数似然函数$l = \Sigma_n logh(y_n) + \Sigma_n(\theta^T x_n y_n - A(\eta))$<br>下面获得Hessian阵：<br>\begin{equation}\begin{split} H&amp;=\frac{d^2 l}{d\theta d\theta^T}\\<br>&amp; = \frac{d}{d\theta^T}\Sigma_n(y_n-\mu_n)x_n\\<br>&amp; = \Sigma_n x_n \frac{d\mu_n}{d\theta^T}\\<br>&amp; = -\Sigma_n X_n \frac{d\mu_n}{d\eta_n} \frac{d\eta_n}{d\theta^T}\\<br>&amp; = -\Sigma_n X_n \frac{d\mu_n}{d\eta_n} x_n^T \   因为\eta_n = \theta^T x_n\\<br>&amp; = -X^T W X<br>\end{split}\end{equation}<br>其中$X = [x_n^T]$，$W = diag[\frac{d\mu_1}{d\eta_1},…,\frac{d\mu_N}{d\eta_N}]$。W可以同过计算$A(\eta)$的二阶导来计算。<br>代换上式中的$\triangledown J(\theta)$和H，可以得到：<br>$$ \theta^{t+1} = (X^T W^t X)^{-1} X^T W^t z^t $$<br>其中$z^t = X\theta^ t + (W^t)^{-1}(y - \mu^t)$。因为W是对角阵，所有该式子具有解耦的作用。</p>
<h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><p>条件概率分布如下（伯努利分布）：<br>$$ p(y \mid x) = \mu(x)^y (1-\mu(x))^{1-y} $$<br>其中$\mu$是logistic函数<br>$$ \mu(x) = \frac{1}{1 + e^{-\eta(x)}} $$</p>
<p>由于$p(y\mid x)$是指数族，<br>均值：<br>$$ E[y\mid x] = \mu = \frac{1}{1 + e^{-\eta(x)}} $$<br>canonical response  function:<br>$$ \eta = \xi = \theta^T x $$<br>利用上面的方法广义线性模型中的方法求W：<br>$$ \frac{d\mu}{d\eta} = \mu (1 - \mu) $$<br>$$ W =<br>\begin{pmatrix}<br>\mu_1 (1 - \mu_1)\\<br>&amp;\ddots \\<br>&amp; &amp; \mu_N (1-\mu_N) \\<br>\end{pmatrix}<br>$$<br>其中N是训练样本的数量，d是输入样本的维度。上面的方法每代复杂度为$O(Nd^3)$。可以利用拟牛顿法来近似计算Hessian阵来减小运算成本。<br>共轭梯度每代的复杂度为$O(N d)$，在实际中使用效果更好。dang样本数量较大时，也可以采用随机梯度下降。</p>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>条件概率分布为：<br>$$p(y\mid x,\theta,\Sigma)=\frac{1}{(2\pi)^{k/2}\Sigma^{\frac{1}{2}}}exp \lbrace  \frac{1}{2}(y-\mu)^T\Sigma^{-1}(y - \mu) \rbrace$$<br>从上面多元正态分布中，可以写成指数族的形式。<br>利用上面的方法广义线性模型中的方法求W：<br>$$ \frac{d\mu}{d\eta} = 1 $$<br>$$ W = 1 $$<br>更新规则如下：<br>$$ \theta^{t+1} = \theta^t + (X^T X)^{-1} X^T(y - \mu^t) $$</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li>对于指数族分布，最大似然估计等价于矩估计。</li>
<li>广义线性模型是图模型的实际应用中的重要组成部分。</li>
<li>要选择合适的独立性以及合适的先验。</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html" target="_blank" rel="noopener">http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html</a><br>注：本文主要参考[1]中第6讲视频以及笔记。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/25/概率论知识点/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/25/概率论知识点/" itemprop="url">概率论知识点</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-25T10:11:46+08:00">
                2018-04-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="频率学派和贝叶斯学派"><a href="#频率学派和贝叶斯学派" class="headerlink" title="频率学派和贝叶斯学派"></a>频率学派和贝叶斯学派</h1><p>频率学派：观测数据是随机变量，参数是未知但确定的；<br>贝叶斯学派：观测数据是已知的，参数是随机变量。<br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/18/神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/18/神经网络/" itemprop="url">神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-18T20:30:14+08:00">
                2018-04-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>感知机主要是解决了与非问题，输入是二值的0或者1，后来出现了Sigmoid。解决了感知机只能处理与非问题，将输出变为0~1之间，这样就有了神经网络。下面的文章非常的通俗易懂，可以一看。<br><a href="http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
      
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/18/message-passing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/18/message-passing/" itemprop="url">message-passing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-18T10:52:19+08:00">
                2018-04-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="变量消除的缺点"><a href="#变量消除的缺点" class="headerlink" title="变量消除的缺点"></a>变量消除的缺点</h1><p>elimination algorihthm中会有clique中重复使用的情况，message passing将重复使用的clique保留下来，这样可以减少运算复杂度。</p>
<h1 id="Elimination-on-a-tree"><a href="#Elimination-on-a-tree" class="headerlink" title="Elimination on a tree"></a>Elimination on a tree</h1><p>将从i开始的变量消除记作$m_{ji}(x_i)$，并且是$x_i$的函数。<br>$$m_{ji}(x_i)=\sum_{x_j}(\psi(x_j) \psi(x_i,x_j)\prod_{k\in N(j)\j} m_{kj}(x_j))$$<br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/elimination%20on%20a%20tree.png" alt=""><br>$m_{ji}(x_i)$能够表示从$x_j$到$x_i$的置信。</p>
<h1 id="Two-pass-Algorithm"><a href="#Two-pass-Algorithm" class="headerlink" title="Two-pass Algorithm"></a>Two-pass Algorithm</h1><p>算法的实施的具体步骤，确定一个根节点，从其他节点中收集信息到这个根节点，然后回到分布的信息。直到某一节点中包含了除却继续传播节点的所有信息，这样就计算信息，然后传播到剩下的节点。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html" target="_blank" rel="noopener">http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html</a><br>[2] <a href="http://people.eecs.berkeley.edu/~jordan/prelims/?C=N;O=A" target="_blank" rel="noopener">http://people.eecs.berkeley.edu/~jordan/prelims/?C=N;O=A</a><br>注：本文主要参考[1]中第5讲视频以及笔记，参考[2]中第4章。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


      
    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="HJY" />
            
              <p class="site-author-name" itemprop="name">HJY</p>
              <p class="site-description motion-element" itemprop="description">HJY的装逼小站</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/hjyai94" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HJY</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.3"></script>



  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
