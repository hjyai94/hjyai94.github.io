<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />










  <meta name="baidu-site-verification" content="i0w0IGpafr" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="HJY的装逼小站">
<meta property="og:type" content="website">
<meta property="og:title" content="HJY">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="HJY">
<meta property="og:description" content="HJY的装逼小站">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HJY">
<meta name="twitter:description" content="HJY的装逼小站">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: 'HR62QHMRVP',
      apiKey: '5003cc57039452aa0e152bdb9198ed17',
      indexName: 'dev_NAME',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>HJY</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">HJY</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">昨夜西风凋碧树，独上高楼望尽天涯路。</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/07/Papers1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/07/Papers1/" itemprop="url">Papers1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-07T16:24:07+08:00">
                2019-04-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/科研/" itemprop="url" rel="index">
                    <span itemprop="name">科研</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文整理了最近看的论文，主要方向是脑肿瘤分割，由论文链接，论文出发点，以及论文的创新点构成。</p>
<h1 id="Densely-Connected-Convolutional-Networks"><a href="#Densely-Connected-Convolutional-Networks" class="headerlink" title="Densely Connected Convolutional Networks"></a>Densely Connected Convolutional Networks</h1><p><a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener">Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.</a></p>
<h2 id="Standpoint"><a href="#Standpoint" class="headerlink" title="Standpoint"></a>Standpoint</h2><p>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output.</p>
<h2 id="Innovation"><a href="#Innovation" class="headerlink" title="Innovation"></a>Innovation</h2><p>Design densely connected convolutional networks with shorter connections.</p>
<h1 id="DRINet-for-Medical-Image-Segmenation"><a href="#DRINet-for-Medical-Image-Segmenation" class="headerlink" title="DRINet for Medical Image Segmenation"></a>DRINet for Medical Image Segmenation</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/8357580" target="_blank" rel="noopener">Chen L, Bentley P, Mori K, et al. DRINet for medical image segmentation[J]. IEEE transactions on medical imaging, 2018, 37(11): 2453-2462.</a></p>
<h2 id="Standpoint-1"><a href="#Standpoint-1" class="headerlink" title="Standpoint"></a>Standpoint</h2><p>These convolution layers learn representative features of input images and construct segmentation based on the features. However, the features learned by standard convolution layers are not distinctive when the differences among different categoriesare subtle in terms of intensity, location,shape, and size.</p>
<h2 id="Innovation-1"><a href="#Innovation-1" class="headerlink" title="Innovation"></a>Innovation</h2><p>A novel combination of the dense connections with the inception structure to address segmentation problems. The use of dense connection blocks, residual inception blocks, and the unpooling blocks achieve high performance while maintaining computational efficiency;</p>
<h1 id="Autofocus-Layer-for-Semantic-Segmentation"><a href="#Autofocus-Layer-for-Semantic-Segmentation" class="headerlink" title="Autofocus Layer for Semantic Segmentation"></a>Autofocus Layer for Semantic Segmentation</h1><p><a href="https://arxiv.org/pdf/1805.08403.pdf" target="_blank" rel="noopener">Qin Y, Kamnitsas K, Ancha S, et al. Autofocus layer for semantic segmentation[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2018: 603-611.</a></p>
<h2 id="Standpoint-2"><a href="#Standpoint-2" class="headerlink" title="Standpoint"></a>Standpoint</h2><p>For high performance, segmentation algorithms are required to use multi-scale context [6], while still aiming for pixel-level accuracy. Multi-scale processing provides detailed cues, such as texture information of a structure, combined with contextual information, such as a structure’s surroundings, which can facilitate decisions that are ambiguous when based only on local context.</p>
<h2 id="Innovation-2"><a href="#Innovation-2" class="headerlink" title="Innovation"></a>Innovation</h2><p>They propose the autofocus convolutional layer for semantic segmentation with the objective of enhancing the capabilities of neural networks for multi-scale processing. Autofocus layers adaptively change the size of the e_ective receptive field based on the processed context to generate more powerful features. This is achieved by parallelising mul-tiple convolutional layers with di_erent dilation rates, combined by an attention mechanism that learns to focus on the optimal scales driven by context.</p>
<h1 id="Efficient-Brain-Tumor-Segmentation-with-Multiscale-Two-Pathway-Group-Conventional-Neural-Networks"><a href="#Efficient-Brain-Tumor-Segmentation-with-Multiscale-Two-Pathway-Group-Conventional-Neural-Networks" class="headerlink" title="Efficient Brain Tumor Segmentation with Multiscale Two-Pathway-Group Conventional Neural Networks"></a>Efficient Brain Tumor Segmentation with Multiscale Two-Pathway-Group Conventional Neural Networks</h1><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481481" target="_blank" rel="noopener">Razzak I, Imran M, Xu G. Efficient brain tumor segmentation with multiscale two-pathway-group conventional neural networks[J]. IEEE journal of biomedical and health informatics, 2018.</a></p>
<h2 id="Standpoint-3"><a href="#Standpoint-3" class="headerlink" title="Standpoint"></a>Standpoint</h2><ol>
<li><p>Manual segmentation of the brain tumors for cancerdiagnosis from MRI images is a difficult, tedious and timeconsuming task. The accuracy and the robustness of brain tumor segmentation, therefore, are crucial for the diagnosis, treatment planning, and treatment outcome evaluation.</p>
</li>
<li><p>Traditional methods of Deep learning such as Convolutional Neural Networks require a large amount of annotated data to learn from, which is often difficult to obtain in the medical domain.</p>
</li>
</ol>
<h2 id="Innovation-3"><a href="#Innovation-3" class="headerlink" title="Innovation"></a>Innovation</h2><p>They describe a new model Two-Pathway-Group CNN architecture for brain tumor segmentation, which exploits local features and global contextual features simultaneously.</p>
<h1 id="Efficient-Multi-scale-3D-CNN-with-Fully-Connected-CRF-for-Accurate-Brain-lesion-Segmentation"><a href="#Efficient-Multi-scale-3D-CNN-with-Fully-Connected-CRF-for-Accurate-Brain-lesion-Segmentation" class="headerlink" title="Efficient Multi-scale 3D CNN with Fully Connected CRF for Accurate Brain lesion Segmentation"></a>Efficient Multi-scale 3D CNN with Fully Connected CRF for Accurate Brain lesion Segmentation</h1><p><a href="https://pdf.sciencedirectassets.com/272154/1-s2.0-S1361841516X00084/1-s2.0-S1361841516301839/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEFoaCXVzLWVhc3QtMSJHMEUCIQCsuiUnmXyD1We5Tahtd1Ldmgji9yslyjwGHfhjWGrTiQIgZI2%2BSICLI%2BYwirHiDv%2FTGDXrqzMMTAnO5VipOujh940q2gMIIxACGgwwNTkwMDM1NDY4NjUiDOhYw6AGmMFoYimUsiq3A%2FnJoakde5OPvuBW9NTk4RD1uQJfiKzq5%2FdL8VVSGt0uujA4qFZjyFNLFImg8WtHa8N72KURPf%2F%2FlECf7quq6thriQ0IWcKRggEmnVyrb5S75twqjs9lMB8wJAS42NwKBogUAnv7a%2FKHlyI8bYvJvCYZfvrlvFkwfoqPN0pvd9qRc22UwUp0osCTlucrynSxWlDakHV20ZOyw985%2FS8ERhfyMNmK%2B3poagUKTzXbwy1oA8CJ8njLfuV7uNiL2GXJrWnv4XD%2FrS7R%2BedMGzNCK%2Fel7pAff6%2B5Q6Mzd4%2FmtnA9UYvyGCJiY4qOlxib%2FG8oFexAZohS20ZIqgpWg4BUz%2FG5%2BqWxHxPB6htlq503UBMvXYqp86NIZ7%2B2tbZ7bKWr7Lj27885H0t6YEQjB4Bec0wVQLnEfpo4vI2b0zxlItdgpALLT9XuYog9rBvCa6G943RX4qA5wO0MtkZl3mcTqgDpImX7qsYnhY9cZWWgOqxYiNX0ksUDoggLES6QI9s1XVaKh4fd0bTQ%2BF9kBw8s%2BJ%2FaVU1TwFWWLJDHmMPwL6ZUV6bvMdmjPcG6aFz3lru8gPARpK0YlmQwu6Ol5QU6tAFf7vS%2B1BXojpfdrukDGETT2BJQlXwWYhY%2F4SH2PnmzSW%2BOWqFiVeuSGtpzWetonvYspUAAt9zO24zb0Ap2SSIzKn8Zd6qnzjEZzI8rLe3i9CmoZ%2BlfC41rZvNeYEqxWapay%2F8ygSZUDYTvHj93Vj4eFdu1uHVL5Tm2vFs8ukNDDlS6hdp3Zk3UaM1QfDkLksa8fEjCwFldyOdIcfhoykJHNM8%2FpdBmA%2Ffqz%2FBk7HaV8kgtL9Y%3D&amp;AWSAccessKeyId=ASIAQ3PHCVTYQ3ZV253X&amp;Expires=1554601073&amp;Signature=OeSwkuAFTu2MZ2agSyHzQz7OZww%3D&amp;hash=0185f4d47582fa2a2ee2c35cf986b0fd9e15d14bd40721c8e67816d7851da17e&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S1361841516301839&amp;tid=spdf-b5e3157c-b347-43ad-bbd6-0d660c93b8f7&amp;sid=2a768add952a0346b77ac6a49311fd650ab7gxrqa&amp;type=client" target="_blank" rel="noopener">Kamnitsas K, Ledig C, Newcombe V F J, et al. Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation[J]. Medical image analysis, 2017, 36: 61-78.</a></p>
<h2 id="Standpoint-4"><a href="#Standpoint-4" class="headerlink" title="Standpoint"></a>Standpoint</h2><p> The heterogeneous appearance of lesions including the large variability in location, size, shape and frequency make it difficult to devise effective segmentation rules. It is thus highly non-trivial to delineate contu- sions, oedema and haemorrhages in TBI, or sub-components of brain tumours such as proliferating cells and necrotic core.</p>
<h2 id="Innovation-4"><a href="#Innovation-4" class="headerlink" title="Innovation"></a>Innovation</h2><p> They  propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. </p>
<h1 id="HyperDense-Net-A-Hyper-densely-Connected-CNN-for-Multi-modal-Image-Segmentation"><a href="#HyperDense-Net-A-Hyper-densely-Connected-CNN-for-Multi-modal-Image-Segmentation" class="headerlink" title="HyperDense-Net: A Hyper-densely Connected CNN for Multi-modal Image Segmentation"></a>HyperDense-Net: A Hyper-densely Connected CNN for Multi-modal Image Segmentation</h1><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8515234" target="_blank" rel="noopener">Dolz J, Gopinath K, Yuan J, et al. HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation[J]. IEEE transactions on medical imaging, 2018.</a></p>
<h2 id="Standpoint-5"><a href="#Standpoint-5" class="headerlink" title="Standpoint"></a>Standpoint</h2><p>Dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet, which connects each layer to every other layer in a feed-forward fashion, has shown impressive performances in natural image classification tasks.</p>
<h2 id="Innovation-5"><a href="#Innovation-5" class="headerlink" title="Innovation"></a>Innovation</h2><p>They propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths.</p>
<h1 id="A-Deep-Learning-model-integrating-FCNNs-and-CRFs-for-brain-tumor-segmentation"><a href="#A-Deep-Learning-model-integrating-FCNNs-and-CRFs-for-brain-tumor-segmentation" class="headerlink" title="A Deep Learning model integrating FCNNs and CRFs for brain tumor segmentation"></a>A Deep Learning model integrating FCNNs and CRFs for brain tumor segmentation</h1><p><a href="https://pdf.sciencedirectassets.com/272154/1-s2.0-S1361841517X0008X/1-s2.0-S136184151730141X/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEFsaCXVzLWVhc3QtMSJHMEUCIQCmFiL2ekwH7KF8z%2Fc0vWR0Ac9yL8K5oGv5DOeBI7VJhQIgfy%2FpbMeHw7IWu30Pc%2FGwfPGxvuI5Calk%2Bgjx2fm5U8wq2gMIJBACGgwwNTkwMDM1NDY4NjUiDIGTbcyCi60TQJ8PRyq3A1%2Fzz%2B3VHHOekK%2FJw6MU%2FZqrd9oySC4b6nKbDevYilRhqIjYFRKlF1Ij0YgzO2xl2qPCJA1Luoh3fIm4CfQomsuNh0vwUU4VcwwTOdRKW4Biycm5IxXEePAh2xuIUSBB%2B%2BRbOIwmmHKraKf8UaAlFLz%2FxDHEPqk%2Ft8WllncOm3fus26FVvSt6tBpAiUmIlWi%2B8a%2BiJ6GF61aZBjlsGbZhDHS%2BtsTXMdRysCnSdGTClkKbVshva2YJUU7dM%2BqzxrrGCTCDCFYq%2Fo%2FCszl%2BTV%2BperpIIltFrMZdVk1g%2FnAmD35O%2Bsgk1V4iQiYrdXAvCcNO1Vt7gmaTg8k4lZjXacSw52vrDW76YcU%2FOWq7BYkFB8v3CTeIH0pPvvDmLRbSOYt7HmYWMuSTR5AS5DgrJvY2al3UczqOK8rYSN4qcu5GdgNa7fzvQaouXF2SHQ%2FVj9sH8agALf0MbOzTOH43EadU5ATM8X3JYwWEh9bP7mPor3cQm1VHvDz0Eq9NtduHt2r9EYQ0nqR22AmBAfSYbaf1yz55k6bwUtqukP5IJm6BCeeaVbaz9zB0rVbAHdUCh3tgnqsds%2Fvlf4wy7%2Bl5QU6tAFpdYJnVeDFfg8QKDDpZ9fcsFs5bdMoGPRaU8hY495aNV4QleUmufJAirYtZvSZAQbZGelsALa1gT7OwuMDACZ76e3FyfsRjc9sgQIWXy73%2FvnbUIiSAH%2F0BUiE0Sfq%2FXB2gzShepFEjd1nIf9nrzmKl8TLiMxWQm%2F7txg8Y4ndrsBnq1YWoYn9nkg6MF4a5Hy6M4W2hBk%2FOq1MpN1%2Bh20l%2FhkH%2BUDkInb5ni4zsRTuPDTvgi0%3D&amp;AWSAccessKeyId=ASIAQ3PHCVTYRA73P2UO&amp;Expires=1554605840&amp;Signature=mupLWqHKaKQuDReHsSReahixCgc%3D&amp;hash=d7e362b71f1842d8657ac4a022fe9b558190b5ecd29082efd89e41cda056cd25&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S136184151730141X&amp;tid=spdf-ff9d6147-ba0b-4dfc-b397-32911281b48b&amp;sid=2a768add952a0346b77ac6a49311fd650ab7gxrqa&amp;type=client" target="_blank" rel="noopener">Zhao X, Wu Y, Song G, et al. A deep learning model integrating FCNNs and CRFs for brain tumor segmentation[J]. Medical image analysis, 2018, 43: 98-111</a></p>
<h2 id="Standpoint-6"><a href="#Standpoint-6" class="headerlink" title="Standpoint"></a>Standpoint</h2><p>Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation.</p>
<h2 id="Innovation-6"><a href="#Innovation-6" class="headerlink" title="Innovation"></a>Innovation</h2><p>Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency.</p>
<h1 id="U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation"><a href="#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="U-Net: Convolutional Networks for Biomedical Image Segmentation"></a>U-Net: Convolutional Networks for Biomedical Image Segmentation</h1><p><a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241.</a></p>
<h2 id="Standpoint-7"><a href="#Standpoint-7" class="headerlink" title="Standpoint"></a>Standpoint</h2><p>There is large consent that successful training of deep networks requires many thousand annotated training samples.</p>
<h2 id="Innovation-7"><a href="#Innovation-7" class="headerlink" title="Innovation"></a>Innovation</h2><p>In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently.</p>
<h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><p>This is architecture is very useful in many medical image segmentation task, so is it the best architecture?</p>
<p>The novel architecture is not emphasized in this paper, but this paper propose U-Net that has been a popular network architecture.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/22/private-jupyter-server/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/22/private-jupyter-server/" itemprop="url">private_jupyter_server</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-22T14:16:40+08:00">
                2019-03-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/28/Brain-tumor-segmentation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/Brain-tumor-segmentation/" itemprop="url">基于卷积神经网络的脑肿瘤分割</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T22:09:38+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本人一直研究脑肿瘤分割，脑肿瘤分割对于患者的后续治疗以及对疾病的检测有着重要的意义，同时也是人工智能处理医学图像的重要方向之一。目前开源的代码都比较复杂，不适合入门研究，另外 Pytorch 作为一个容易上手的深度学习框架，具有很强的灵活性，适合新手或者是科研工作者，所以本文的代码将使用深度学习框架 Pytorch1.0 和 Python3.6 进行编程构建卷积神经网络来进行脑肿瘤分割。卷积神经网络不仅在自然图像而且在医学图像在内的其他图像都有着广泛地应用。另外，卷积神经网络广泛地应用于图像的分类，检测等计算机视觉任务中。</p>
<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><p>我们这里使用 BraTs2015 的部分数据集，数据集可以从<a href="https://github.com/yaq007/Autofocus-Layer" target="_blank" rel="noopener">这里下载</a>，完整的BraTs2015 数据集可以在这里<a href="https://www.smir.ch/BRATS/Start2015" target="_blank" rel="noopener">注册下载</a>。</p>
<p>本文使用的数据集共有20例样本用于训练，54例样本用于测试(可自行调整)，每个样本中共有4个模态的数据和Mask和真值数据，其中4个模态分别为FLAIR， T1，T1c，T2。真值数据共有5个标签：</p>
<ul>
<li>label 1: necrosis</li>
<li>label 2: edema</li>
<li>label 3: non-enhacing tumor</li>
<li>label 4: enhancing tumor</li>
<li>label 0：everything else</li>
</ul>
<p>脑肿瘤分割主要有3个部分，Whole tumor， Tumor core， Enhance tumor。这3个部分的标签如下所示：</p>
<ul>
<li>Whole tumor: label 1, 2, 3, 4</li>
<li>Enhance tumor: label 4</li>
<li>Tumor core: 1, 3, 4</li>
</ul>
<p>BraTs2015 使用 Dice 作为评价指标，这个评价指标主要是衡量预测结果与真值之间重叠部分。 Dice 的公式计算如下：<br>$$ Dice = \frac{2 TP}{2TP + FP + FN} $$</p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>前面我们知道 BraTs 2015 共有4个模态的数据，下面我们介绍两个能够在程序中读取医学图像的包：SimpleITK 和 Nibabel。SimpleITK能够读取的格式更加多，具体可以参考 SimpleITK 的文档。</p>
<h3 id="SimpleITK"><a href="#SimpleITK" class="headerlink" title="SimpleITK"></a>SimpleITK</h3><p>SimpleITK 读取医学图像示例代码：<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> SimpleITK as sitk</span><br><span class="line"><span class="built_in">image</span> = sitk.ReadImage(<span class="string">'image.nii'</span>)</span><br><span class="line"><span class="built_in">image</span> = sitk.GetArrayFromImage(<span class="built_in">image</span>)</span><br></pre></td></tr></table></figure></p>
<p>SimpleITK 读取的图片维度是通道优先的，所以图像的维度的第1位是医学图像的维数。另外我还使用到了 SimpleITK 将输出预测结果保存为医学图像格式，这部分代码如下：<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">image</span> = sitk.GetImageFromArray(<span class="built_in">image</span>)</span><br><span class="line">sitk.WriteImage(<span class="built_in">image</span>, <span class="string">'image.mha'</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="Nibabel"><a href="#Nibabel" class="headerlink" title="Nibabel"></a>Nibabel</h3><p>Nibabel 读取医学图像示例代码：<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nibabel as nib </span><br><span class="line"><span class="built_in">image</span> = nib.load(<span class="string">'image.nii.gz'</span>).get_fdata()</span><br><span class="line"><span class="built_in">image</span> = <span class="built_in">image</span>.transpse(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<p>不同于 SimpleITK， Nibabel 读取图像的维度的通道数位于最后 1 位，但是 Nibabel 将图像旋转了 $90^{\circ}$，可以使用上面代码的第三行旋转为一般维数分布方式。比如脑图中，如果不旋转变换转变 Nibabel 读取的方式，脑图就是横着的。</p>
<h3 id="3D-Slicer-可视化"><a href="#3D-Slicer-可视化" class="headerlink" title="3D Slicer 可视化"></a>3D Slicer 可视化</h3><p>SimpleITK 和 Nibabel 是可以在程序中读取医学图像的包，灵活性不强，另外不适合了解医学图像的基本特性，下面我们使用 3D Slicer 来可视化我们的医学图像数据。<br><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/FLAIR_axis.png" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/FLAIR_sagittal.png" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/FLAIR_coronal.png" alt=""></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/T1_axis.png" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/t1_sagittal.png" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/T1_coronal.png" alt=""></div></div></div></div><br>上图中，第一行为 FLAIR 模态，从左到有依次为横断面(Axis plane)，矢状面(Sagittal plane)，冠状面(Coronal plane)，第二行为 T1 模态，从左到右顺序与 FLAIR 相同，另外两个模态因为篇幅的关系不做具体地展示。下面是脑图的 mask 和手工分割肿瘤的真值(可以看做是金标准，但是个人认为还是有区别的)：<br><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/mask_axis.png" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/mask_sagittal.png" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/mask_coronal.png" alt=""></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/ground_truth_axis.png" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/ground_truth_sagittal.png" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/ground_truth_coronal.png" alt=""></div></div></div></div><br>本人使用的数据库来自于项目[2] 中，具体如何得到的 mask 数据并不是很清楚，不过通过 3D slicer 可以手工生成mask数据，这里时间有限，不准备写了。上图中第二行是三个维度的真值图，其中外围浅蓝色为剔除肿瘤之外的区域(label 0)，绿色为 edema(label 2)，红色为 enhance tumor(label 4)，深蓝色区域为 necrosis(label 1)，黄色区域为 non-enhance tumor(label 3)。注：这里使用的 Brats 2015 采用了这5个标签，之后的该数据集进行了调整，将non-enhncing 和 necrosis 合并为 label 1。</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>本文使用卷积神经网络，主要结构参考[1]中的结构，不同之处在于为了方便理解，我们只是用了一条通道。模型如下图所示：<br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/brain_tumor_segmentation_CNN/brain_tumor_segmentation_model.png" alt=""><br>训练使用交叉熵作为损失函数，利用 RMSprop 作为优化器，学习率设为 $1e-3$，可以对学习率随着epoch进行调整，这里没有改变，读者可以根据自己的想法进行调整。具体代码可以在我的github[3]中查看，喜欢记得点个小星星。</p>
<h1 id="训练与测试"><a href="#训练与测试" class="headerlink" title="训练与测试"></a>训练与测试</h1><p>训练部分采用随机裁剪图片大小为 $75\times75\times75$ ，输入到神经网络中，最后得到$47\times47\times47$ (因为卷积中没有采用padding，所以出现了输出小于输入的情况)。测试部分使用完全大小的图像 $240 \times 240 \times 225$，输入到网络中得到大小为$212 \times 212 \times 197$，因为脑图的边缘是无效的信息，所以输出的大小与输入和真值维度不一致，不会有影响，只需要补全就行了。</p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>最后网络在三个子区域的分割指标 Dice 为：Whole score 0.6764， Enhancing tumor 0.4478, Tumor core 0.4819，这个结果并不是非常的好，读者如需更高准确度的分割结果，可以调整测试时候的输入图像的维数，将其保持为与训练时输入的图像维数一致，另外可以参考最新的脑肿瘤分割的工作，对代码进行改进。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Kamnitsas K, Ledig C, Newcombe V F J, et al. Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation[J]. Medical image analysis, 2017, 36: 61-78.<br>[2] <a href="https://github.com/yaq007/Autofocus-Layer" target="_blank" rel="noopener">https://github.com/yaq007/Autofocus-Layer</a><br>[3] <a href="https://github.com/hjyai94/Half_Pathway_DeepMedic" target="_blank" rel="noopener">https://github.com/hjyai94/Half_Pathway_DeepMedic</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/07/医学图像可视化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/07/医学图像可视化/" itemprop="url">医学图像可视化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-07T15:43:57+08:00">
                2019-01-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近经常使用到之前写的对医学图像进行可视化，下面我将这部分代码和相关内容整理成该博客，分成一下三个部分，第一部分是医学图像的格式，第二部分是医学图像的读取，第三部分是医学图像的可视化。</p>
<h1 id="医学图像格式"><a href="#医学图像格式" class="headerlink" title="医学图像格式"></a>医学图像格式</h1><p>医学图像常见的有6种主要格式，分别为DICOM(医学数字成像和通讯)、NIFTI(神经影像信息技术)、PAR/REC(Philips磁共振扫描格式)、ANALYZE(Mayo医学成像)、NRRD(近原始栅格数据)和MNIC。<br>目前我就处理过DICOM和NIFTI格式的数据，下面主要对这两种格式进行解释。</p>
<h2 id="DICOM"><a href="#DICOM" class="headerlink" title="DICOM"></a>DICOM</h2><h2 id="NIFTI"><a href="#NIFTI" class="headerlink" title="NIFTI"></a>NIFTI</h2><p>s</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/07/进化算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/07/进化算法/" itemprop="url">进化算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-07T20:30:01+08:00">
                2018-12-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/编程/" itemprop="url" rel="index">
                    <span itemprop="name">编程</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="准备在博客中新开一个分类，这个分类将记录一些实现的代码，又从别人那里借鉴来的，或者是自己写的之类的。"><a href="#准备在博客中新开一个分类，这个分类将记录一些实现的代码，又从别人那里借鉴来的，或者是自己写的之类的。" class="headerlink" title="准备在博客中新开一个分类，这个分类将记录一些实现的代码，又从别人那里借鉴来的，或者是自己写的之类的。"></a>准备在博客中新开一个分类，这个分类将记录一些实现的代码，又从别人那里借鉴来的，或者是自己写的之类的。</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/29/CRF进行图像分割/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/29/CRF进行图像分割/" itemprop="url">CRF进行图像分割</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-29T15:58:49+08:00">
                2018-11-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在前面的话"><a href="#写在前面的话" class="headerlink" title="写在前面的话"></a>写在前面的话</h1><p>最近，十分困惑条件随机场是如何工作的，为什么可以加在卷积神经网络的后面作为后处理的部分。虽然理论部分前面的博客也有写过，做过一些总结，不过因为没有实现过代码，所以仍有困惑解决不了，每念至此，心绪不宁，遂作此文，以供参考。</p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p>本文将实现全连接随机场对非RGB的图像进行分割，主要参考文献[1]以及对应的<a href="https://github.com/lucasb-eyer/pydensecrf" target="_blank" rel="noopener">github</a>代码，另外本文需要安装pydensecrf，可以通过<code>pip install pydensecrf</code>安装，安装时需注意，pydensecrf依赖于cython，需要先安装cython。</p>
<h2 id="对非RGB图像分割"><a href="#对非RGB图像分割" class="headerlink" title="对非RGB图像分割"></a>对非RGB图像分割</h2><p>本文的代码放在了我的github中命名为CRF的仓库库中，<a href="https://github.com/hjyai94/CRF/blob/master/examples/Non%20RGB%20Example.ipynb" target="_blank" rel="noopener">链接地址</a>，这里的代码来自于<a href="https://github.com/lucasb-eyer/pydensecrf" target="_blank" rel="noopener">pydensecrf</a>。</p>
<h3 id="一元势"><a href="#一元势" class="headerlink" title="一元势"></a>一元势</h3><p>一元势包含了每个像素对应的类别，这些可以来自随机森林或者是深度神经网络的softmax。这里，我们共有两个类别，一个是前景，一个是背景，这里大小设置为$400\times 512$。我们建立了两个二维的高斯分布，并且平面显示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> multivariate_normal</span><br><span class="line"></span><br><span class="line">H, W, NLABELS = <span class="number">400</span>, <span class="number">512</span>, <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This creates a gaussian blob...</span></span><br><span class="line">pos = np.stack(np.mgrid[<span class="number">0</span>:H, <span class="number">0</span>:W], axis=<span class="number">2</span>)</span><br><span class="line">print(pos.shape)</span><br><span class="line">rv = multivariate_normal([H//<span class="number">2</span>, W//<span class="number">2</span>], (H//<span class="number">4</span>)*(W//<span class="number">4</span>))</span><br><span class="line">probs = rv.pdf(pos)</span><br><span class="line">print(probs.shape)</span><br><span class="line"><span class="comment"># ...which we project into the range [0.4, 0.6]</span></span><br><span class="line">probs = (probs-probs.min()) / (probs.max()-probs.min())</span><br><span class="line">probs = <span class="number">0.5</span> + <span class="number">0.2</span> * (probs<span class="number">-0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The first dimension needs to be equal to the number of classes.</span></span><br><span class="line"><span class="comment"># Let's have one "foreground" and one "background" class.</span></span><br><span class="line"><span class="comment"># So replicate the gaussian blob but invert it to create the probability</span></span><br><span class="line"><span class="comment"># of the "background" class to be the opposite of "foreground".</span></span><br><span class="line">probs = np.tile(probs[np.newaxis,:,:],(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">probs[<span class="number">1</span>,:,:] = <span class="number">1</span> - probs[<span class="number">0</span>,:,:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let's have a look:</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>); plt.imshow(probs[<span class="number">0</span>,:,:]); plt.title(<span class="string">'Foreground probability'</span>); plt.axis(<span class="string">'off'</span>); plt.colorbar();</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>); plt.imshow(probs[<span class="number">1</span>,:,:]); plt.title(<span class="string">'Background probability'</span>); plt.axis(<span class="string">'off'</span>); plt.colorbar();</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/CRF%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/output_9_1.png" alt="output_9_1.png"></p>
<h3 id="使用一元势进行推断"><a href="#使用一元势进行推断" class="headerlink" title="使用一元势进行推断"></a>使用一元势进行推断</h3><p>这里我们可以使用一元势进行推断，也就是说这里我们不考虑像素间的相互关联。这样做并不是很好的推断，但是可以这么做。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inference without pair-wise terms</span></span><br><span class="line">U = unary_from_softmax(probs)  <span class="comment"># <span class="doctag">note:</span> num classes is first dim</span></span><br><span class="line">d = dcrf.DenseCRF2D(W, H, NLABELS)</span><br><span class="line">d.setUnaryEnergy(U)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run inference for 10 iterations</span></span><br><span class="line">Q_unary = d.inference(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Q is now the approximate posterior, we can get a MAP estimate using argmax.</span></span><br><span class="line">map_soln_unary = np.argmax(Q_unary, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Unfortunately, the DenseCRF flattens everything, so get it back into picture form.</span></span><br><span class="line">map_soln_unary = map_soln_unary.reshape((H,W))</span><br><span class="line"><span class="comment"># And let's have a look.</span></span><br><span class="line">plt.imshow(map_soln_unary); plt.axis(<span class="string">'off'</span>); plt.title(<span class="string">'MAP Solution without pairwise terms'</span>);</span><br></pre></td></tr></table></figure></p>
<p><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/CRF%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/output_12_0.png" alt="output_12_0.png"></p>
<h3 id="二元势"><a href="#二元势" class="headerlink" title="二元势"></a>二元势</h3><p>图像处理中，我们经常使用像素间的双边关系，也就是说，我们认为有相似颜色的或者是相似的位置的像素认为是同一类。下面我们建立这样的双边关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">NCHAN=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create simple image which will serve as bilateral.</span></span><br><span class="line"><span class="comment"># Note that we put the channel dimension last here,</span></span><br><span class="line"><span class="comment"># but we could also have it be the first dimension and</span></span><br><span class="line"><span class="comment"># just change the `chdim` parameter to `0` further down.</span></span><br><span class="line">img = np.zeros((H,W,NCHAN), np.uint8)</span><br><span class="line">img[H//<span class="number">3</span>:<span class="number">2</span>*H//<span class="number">3</span>,W//<span class="number">4</span>:<span class="number">3</span>*W//<span class="number">4</span>,:] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.imshow(img[:,:,<span class="number">0</span>]); plt.title(<span class="string">'Bilateral image'</span>); plt.axis(<span class="string">'off'</span>); plt.colorbar();</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the pairwise bilateral term from the above image.</span></span><br><span class="line"><span class="comment"># The two `s&#123;dims,chan&#125;` parameters are model hyper-parameters defining</span></span><br><span class="line"><span class="comment"># the strength of the location and image content bilaterals, respectively.</span></span><br><span class="line">pairwise_energy = create_pairwise_bilateral(sdims=(<span class="number">10</span>,<span class="number">10</span>), schan=(<span class="number">0.01</span>,), img=img, chdim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pairwise_energy now contains as many dimensions as the DenseCRF has features,</span></span><br><span class="line"><span class="comment"># which in this case is 3: (x,y,channel1)</span></span><br><span class="line">img_en = pairwise_energy.reshape((<span class="number">-1</span>, H, W))  <span class="comment"># Reshape just for plotting</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>); plt.imshow(img_en[<span class="number">0</span>]); plt.title(<span class="string">'Pairwise bilateral [x]'</span>); plt.axis(<span class="string">'off'</span>); plt.colorbar();</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>); plt.imshow(img_en[<span class="number">1</span>]); plt.title(<span class="string">'Pairwise bilateral [y]'</span>); plt.axis(<span class="string">'off'</span>); plt.colorbar();</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>); plt.imshow(img_en[<span class="number">2</span>]); plt.title(<span class="string">'Pairwise bilateral [c]'</span>); plt.axis(<span class="string">'off'</span>); plt.colorbar();</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/CRF%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/output_17_0.png" alt="output_17_0.png"><br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/CRF%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/output_18_0.png" alt="output_17_0.png"></p>
<h3 id="使用完整的条件随机场进行推断"><a href="#使用完整的条件随机场进行推断" class="headerlink" title="使用完整的条件随机场进行推断"></a>使用完整的条件随机场进行推断</h3><p>下面我们将一元势与二元势结合起来进行推断，执行不同的迭代次数，有下面的结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">d = dcrf.DenseCRF2D(W, H, NLABELS)</span><br><span class="line">d.setUnaryEnergy(U)</span><br><span class="line">d.addPairwiseEnergy(pairwise_energy, compat=<span class="number">10</span>)  <span class="comment"># `compat` is the "strength" of this potential.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This time, let's do inference in steps ourselves</span></span><br><span class="line"><span class="comment"># so that we can look at intermediate solutions</span></span><br><span class="line"><span class="comment"># as well as monitor KL-divergence, which indicates</span></span><br><span class="line"><span class="comment"># how well we have converged.</span></span><br><span class="line"><span class="comment"># PyDenseCRF also requires us to keep track of two</span></span><br><span class="line"><span class="comment"># temporary buffers it needs for computations.</span></span><br><span class="line">Q, tmp1, tmp2 = d.startInference()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    d.stepInference(Q, tmp1, tmp2)</span><br><span class="line">kl1 = d.klDivergence(Q) / (H*W)</span><br><span class="line">map_soln1 = np.argmax(Q, axis=<span class="number">0</span>).reshape((H,W))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    d.stepInference(Q, tmp1, tmp2)</span><br><span class="line">kl2 = d.klDivergence(Q) / (H*W)</span><br><span class="line">map_soln2 = np.argmax(Q, axis=<span class="number">0</span>).reshape((H,W))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">    d.stepInference(Q, tmp1, tmp2)</span><br><span class="line">kl3 = d.klDivergence(Q) / (H*W)</span><br><span class="line">map_soln3 = np.argmax(Q, axis=<span class="number">0</span>).reshape((H,W))</span><br><span class="line"></span><br><span class="line">img_en = pairwise_energy.reshape((<span class="number">-1</span>, H, W))  <span class="comment"># Reshape just for plotting</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>); plt.imshow(map_soln1);</span><br><span class="line">plt.title(<span class="string">'MAP Solution with DenseCRF\n(5 steps, KL=&#123;:.2f&#125;)'</span>.format(kl1)); plt.axis(<span class="string">'off'</span>);</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>); plt.imshow(map_soln2);</span><br><span class="line">plt.title(<span class="string">'MAP Solution with DenseCRF\n(20 steps, KL=&#123;:.2f&#125;)'</span>.format(kl2)); plt.axis(<span class="string">'off'</span>);</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>); plt.imshow(map_soln3);</span><br><span class="line">plt.title(<span class="string">'MAP Solution with DenseCRF\n(75 steps, KL=&#123;:.2f&#125;)'</span>.format(kl3)); plt.axis(<span class="string">'off'</span>);</span><br></pre></td></tr></table></figure></p>
<p><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/CRF%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/output_21_0.png" alt="output_21_0.png"></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Krähenbühl P, Koltun V. Efficient inference in fully connected crfs with gaussian edge potentials[C]//Advances in neural information processing systems. 2011: 109-117.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/27/KL散度/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/27/KL散度/" itemprop="url">KL散度</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-27T15:59:48+08:00">
                2018-11-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>很久没有推导过公式了，感觉水平退步显著，今日看变分推断内容，看到了计算两个高斯分布间的KL散度，下面我自己推导了一下。</p>
<h1 id="高斯分布间的KL散度"><a href="#高斯分布间的KL散度" class="headerlink" title="高斯分布间的KL散度"></a>高斯分布间的KL散度</h1><p>现有先验分布$p_{\theta}(z) = \boldsymbol{N}(0, \boldsymbol{I})$，后验分布$q_{\phi}(\boldsymbol{z}\mid \boldsymbol{x}^{(i)})$同样是高斯分布。变量$z$的维数是$J$。其中，$\boldsymbol{u}$和$\boldsymbol{\sigma}$记作点$i$的均值和标准差。另外，$\mu_j$和$\sigma_j$是均值和方差向量的第$j$个因子。<br>KL散度的公式如下：<br>\begin{equation}\begin{split}<br>D_{KL}(q_{\phi}(\boldsymbol{z})|| p_{\theta}(\boldsymbol{z})) &amp;= \int q_{\phi}(\boldsymbol{z}) log \frac{q_{\phi}(\boldsymbol{z})} {p_{\theta}(\boldsymbol{z})} d\boldsymbol{z}\\<br>&amp;= \int q_{\phi}(\boldsymbol{z}) log q_{\phi}(\boldsymbol{z}) d\boldsymbol{z} - \int q_{\phi}(\boldsymbol{z}) log p_{\theta}(\boldsymbol{z}) d\boldsymbol{z} \\<br>\end{split}\end{equation}<br>第二项如下所示(因为先写的第二项，小声bb.jpg)：<br>\begin{equation}\begin{split}<br>\int q_{\phi}(\boldsymbol{z}) log p_{\theta}(\boldsymbol{z}) d\boldsymbol{z} &amp;= \int \mathcal{N}(\boldsymbol{z;\mu, \sigma^2}) log \mathcal{N}(\boldsymbol{z; 0, I})d \boldsymbol{z} \\<br>&amp;= \int -\frac{1}{2} log{2\pi}\ q_{\phi}(\boldsymbol{z}) -\frac{z^2}{2} q_{\phi}(\boldsymbol{z}) d\boldsymbol{z} \\<br>&amp;= -\frac{J}{2}log(2\pi) -\frac{1}{2} \int q_{\phi}(\boldsymbol{z}) \boldsymbol{z}^2 d\boldsymbol{z} \\<br>&amp;= -\frac{J}{2}log(2\pi) -\frac{1}{2} \int q_{\phi}(\boldsymbol{z}) (\boldsymbol{z- \mu + \mu})^2 d\boldsymbol{z} \\<br>&amp;= -\frac{J}{2}log(2\pi) -\frac{1}{2} \int q_{\phi}(\boldsymbol{z}) [(\boldsymbol{z- \mu})^2 + \boldsymbol{\mu}^2 +2(\boldsymbol{z - \mu})\boldsymbol{\mu}] d\boldsymbol{z} \\<br>&amp;= -\frac{J}{2}log(2\pi) -\frac{1}{2} \int q_{\phi}(\boldsymbol{z}) [(\boldsymbol{z- \mu})^2 + \boldsymbol{\mu}^2)] d\boldsymbol{z} \\<br>&amp;= -\frac{J}{2}log(2\pi) -\frac{1}{2} \int q_{\phi}(\boldsymbol{z}) (\boldsymbol{\sigma}^2 + \boldsymbol{\mu}^2) d\boldsymbol{z} \\<br>&amp;= -\frac{J}{2}log(2\pi) -\frac{1}{2} \sum_{j=1}^{J} (\mu_j^2 + \sigma_j^2)<br>\end{split}\end{equation}</p>
<p>同样，第一项可以写成如下的形式：<br>\begin{equation}\begin{split}<br>\int q_{\phi}(\boldsymbol{z}) log q_{\phi}(\boldsymbol{z}) d\boldsymbol{z} &amp;= -\frac{J}{2}log(2\pi) -\frac{1}{2} \sum_{j=1}^{J} (1 + log\ \sigma_j^2)<br>\end{split}\end{equation}<br>将上面两项合并一起：<br>\begin{equation}\begin{split}<br>D_{KL}(q_{\phi}(\boldsymbol{z}) || p_{\theta}(\boldsymbol{z})) &amp;= \int q_{\phi}(\boldsymbol{z}) log (q_{\phi}(\boldsymbol{z}) - p_{\theta}(\boldsymbol{z})) d\boldsymbol{z}\\<br>&amp;= -\frac{1}{2} \sum_{j=1}^{J} (1 + log\ \sigma_j^2) + \frac{1}{2} \sum_{j=1}^{J} (\mu_j^2 + \sigma_j^2) \\<br>&amp;= \frac{1}{2} \sum_{j=1}^{J} (-1 - log\ \sigma_j^2 + \mu_j^2 + \sigma_j^2)<br>\end{split}\end{equation}</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>Kingma D P. Variational inference &amp; deep learning: A new synthesis[D]. 2017.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/18/MRI读取与可视化I/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/18/MRI读取与可视化I/" itemprop="url">MRI读取与可视化I</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-18T19:00:40+08:00">
                2018-10-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>今天在网上看了一些读取MRI文件的方法，中文的博客并不是很多，另外很多并不适合我的文件格式，本文主要是针对MRI中采用NIFTI(.nii.g其中gz是压缩文件)格式的文件，并进行可视化分析。</p>
<h1 id="NIBabel"><a href="#NIBabel" class="headerlink" title="NIBabel"></a>NIBabel</h1><p>NIBabel是一个常见的读写神经医学文件的python库，包括ANALYZE, GIFTI, NIfTI2, MINC1, MINC2, MGH和ECAT，还有Philips PAR/REC。<br>下面我们用一张大脑的<a href="http://nipy.org/nibabel/_downloads/someones_epi.nii.gz" target="_blank" rel="noopener">MRI图片</a>，来说这个库的使用，以及MRI文件的格式。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nibabel <span class="keyword">as</span> nib</span><br><span class="line">img = nib.load(<span class="string">'downloads/someones_epi.nii.gz'</span>)</span><br><span class="line">img_header = img.get_header()</span><br><span class="line">print(<span class="string">'Header: '</span>, img_header)</span><br><span class="line">img_data = img.get_fdata()</span><br><span class="line">print(<span class="string">'img_data shape: '</span>, img_data.shape)</span><br></pre></td></tr></table></figure></p>
<p>一个格式为NIFTI的格式文件，通常包括头文件和相应的图像文件。图像文件时三维的，上面的MRI图片的shape为(53, 61, 33)。下面我们将三个维度的中间slice进行可视化(因为不会直接将三维的图像可视化)。</p>
<h1 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h1><p>下面我们显示中间的slice：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.imshow(img_data[<span class="number">26</span>, :, :])</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">plt.imshow(img_data[:, <span class="number">30</span>, :])</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">plt.imshow(img_data[:, :, <span class="number">15</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h1 id="下面的工作"><a href="#下面的工作" class="headerlink" title="下面的工作"></a>下面的工作</h1><p>利用pytorch读取文件，训练神机网络。(可能不会上传到本博客中)</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] <a href="http://nipy.org/nibabel/coordinate_systems.html#voxel-coordinates-are-coordinates-in-the-image-data-array" target="_blank" rel="noopener">http://nipy.org/nibabel/coordinate_systems.html#voxel-coordinates-are-coordinates-in-the-image-data-array</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/13/Pytorch_Tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/13/Pytorch_Tutorial/" itemprop="url">Pytorch Tutorial</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-13T00:00:00+08:00">
                2018-10-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在前面的话"><a href="#写在前面的话" class="headerlink" title="写在前面的话"></a>写在前面的话</h1><p>本文主要是基于Pytorch给出的官方<a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">Tutorial</a>，然后我按照自己的喜好编辑成Jupyter文档，后转成本博客，用来作为自己的日常参照材料。</p>
<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><p>Pytorch给我的感觉是：它是基于更高级的封装，实现深度学习更加简单，比较适合科研型的或者是实现一些想法的入门级选手。Tensorflow更适合工程项目，能够比较高效的运行。但是对于一般选手来说，Pytorch更适合，因为它是动态图，在个人代码水平不是很高的情况下，Pytorch的效率是高于Tensorflow的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device( <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Contruct a 2x1 matric, uninitialized:</span></span><br><span class="line">x = torch.empty(<span class="number">2</span>, <span class="number">1</span>, device=device)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.0000],
        [0.0000]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Construct a randomly initialized matrix:</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>, device=device)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.2854, 0.5359, 0.7811],
        [0.1065, 0.0246, 0.3945],
        [0.8341, 0.6808, 0.4578],
        [0.4257, 0.7255, 0.3597],
        [0.3510, 0.3170, 0.1526]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Construct a matrix filled zeros and dtype long</span></span><br><span class="line">x = torch.zeros(<span class="number">2</span>, <span class="number">1</span>, dtype=torch.long, device=device)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0],
        [0]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Construct a tensor directly from data;</span></span><br><span class="line">x = torch.tensor([<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([3, 3])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">2</span>, <span class="number">1</span>, dtype=torch.double) <span class="comment"># new_* methods take in size</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float) <span class="comment"># override dype and result has same size</span></span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1.],
        [1.]], dtype=torch.float64)
tensor([[1.4535],
        [0.0968]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get its size</span></span><br><span class="line">print(x.size())</span><br><span class="line"></span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 1])
torch.Size([2, 1])
</code></pre><h1 id="Opertions"><a href="#Opertions" class="headerlink" title="Opertions"></a>Opertions</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Addition</span></span><br><span class="line">x = x.new_ones(<span class="number">3</span>, <span class="number">2</span>, dtype=torch.float)</span><br><span class="line">y = torch.rand(<span class="number">3</span>, <span class="number">2</span>, dtype=torch.float)</span><br><span class="line">print(x + y) <span class="comment"># syntax 1</span></span><br><span class="line">print(torch.add(x, y)) <span class="comment"># syntax 2</span></span><br><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">torch.add(x, y, out=result) <span class="comment"># add x and y to result</span></span><br><span class="line">print(result)</span><br><span class="line">y.add_(x)</span><br><span class="line">print(y) <span class="comment"># add x to y</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1.9447, 1.9085],
        [1.3177, 1.8074],
        [1.1208, 1.8663]])
tensor([[1.9447, 1.9085],
        [1.3177, 1.8074],
        [1.1208, 1.8663]])
tensor([[1.9447, 1.9085],
        [1.3177, 1.8074],
        [1.1208, 1.8663]])
tensor([[1.9447, 1.9085],
        [1.3177, 1.8074],
        [1.1208, 1.8663]])
</code></pre><p>注：任何tensor后面带有下划线都会改变tensor的值，比如x.copy_(y), x.t_(x)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.8368, 0.9204],
        [0.3797, 0.0908],
        [0.4454, 0.7684]])
tensor([0.9204, 0.0908, 0.7684])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># resizing</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>) <span class="comment"># the size -1 is inferred from other dimensions</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Converting Numpy Array to Torch Tensor</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CUDA Tensor</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">    y = torch.ones_like(x, device=device)</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))</span><br></pre></td></tr></table></figure>
<h1 id="Define-the-Network"><a href="#Define-the-Network" class="headerlink" title="Define the Network"></a>Define the Network</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>
<p>本来这个网络应该是LeNet的，输入要求是32x32，我改变了第一个全连接层，将输入变为了20x20。<br>所以网络结构可以通过自己的想法进行改变，最重要的是改变全连接层就可以了。<br>网络经过卷积之后输入的结果公式为：$$ outputsize = （inputsize - kernelsize + 2 * pad）/stride + 1 $$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">2</span> * <span class="number">2</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># if the size is a square you can only specify a single number </span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_feature(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = F.relu(self.fc3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_feature</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:] <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s </span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>
<pre><code>Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=64, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The learnable parameter of a model are returned by net.parameters()</span></span><br><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size()) <span class="comment"># conv1's .weights</span></span><br></pre></td></tr></table></figure>
<pre><code>10
torch.Size([6, 1, 5, 5])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.0432, 0.1072, 0.0000, 0.1096, 0.0378, 0.0000, 0.0000, 0.0000, 0.0202,
         0.0000]], grad_fn=&lt;ReluBackward&gt;)
</code></pre><h2 id="zero-the-gradients"><a href="#zero-the-gradients" class="headerlink" title="zero the gradients"></a>zero the gradients</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.<br>For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.<br>If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">out = net(input)</span><br><span class="line">target = torch.randn(<span class="number">10</span>) <span class="comment"># a dummy target, for example </span></span><br><span class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(out, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(1.7536, grad_fn=&lt;MseLossBackward&gt;)
</code></pre><p>forward:<br>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d<br>      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear<br>      -&gt; MSELoss<br>      -&gt; loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn) <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>]) <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>]) <span class="comment"># relu</span></span><br></pre></td></tr></table></figure>
<pre><code>&lt;MseLossBackward object at 0x0000000008619780&gt;
&lt;ReluBackward object at 0x0000000008619A58&gt;
&lt;ThAddmmBackward object at 0x0000000008619780&gt;
</code></pre><h2 id="Backpropagate"><a href="#Backpropagate" class="headerlink" title="Backpropagate"></a>Backpropagate</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad() <span class="comment"># zeros the gradient buffer of all parameters </span></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss.backward() # 为了和下面的产生两次相同的backpropagate，所以将这里注释，下一个单元也是这样。</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>
<pre><code>conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Weights</span></span><br><span class="line"><span class="comment"># learning_rate = 0.01</span></span><br><span class="line"><span class="comment"># for f in net.parameters():</span></span><br><span class="line"><span class="comment">#     f.data.sub_(f.grad.data * learning_rate)  # f = f - learning_rate * gradient</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># creater your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop </span></span><br><span class="line">optimizer.zero_grad() <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(out, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step() <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>
<h1 id="Training-A-Classifier"><a href="#Training-A-Classifier" class="headerlink" title="Training A Classifier"></a>Training A Classifier</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The output of torchvision datasets are PILImage iamges of range [0, 1]. We transform them to Tensor of normalized range [-1, 1]</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">[transforms.ToTensor(), transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>, </span><br><span class="line">                                       download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>, </span><br><span class="line">                                          shuffle=<span class="keyword">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>, </span><br><span class="line">                                      download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>, </span><br><span class="line">                                              shuffle=<span class="keyword">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, </span><br><span class="line">          <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Files already downloaded and verified
Files already downloaded and verified
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img/<span class="number">2</span> + <span class="number">0.5</span> <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line"><span class="comment">#     print(npimg.shape)</span></span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"><span class="comment">#     print(np.transpose(npimg, (1, 2, 0)).shape)</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"><span class="comment"># print(labels)</span></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images)) <span class="comment"># 制作图像网格</span></span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/Pytorch_Tutorial_output_35_0%20.png" alt=""></p>
<pre><code>deer truck plane horse
</code></pre><h2 id="Define-a-Convolution-Neural-Network"><a href="#Define-a-Convolution-Neural-Network" class="headerlink" title="Define a Convolution Neural Network"></a>Define a Convolution Neural Network</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">net.to(device)</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>
<pre><code>Net(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
</code></pre><h2 id="Define-a-Loss-function-and-opotimizer"><a href="#Define-a-Loss-function-and-opotimizer" class="headerlink" title="Define a Loss function and opotimizer"></a>Define a Loss function and opotimizer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Train-the-network"><a href="#Train-the-network" class="headerlink" title="Train the network"></a>Train the network</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs </span></span><br><span class="line">        inputs, labels = data </span><br><span class="line">        inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># forward  + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>: <span class="comment"># print every 2000 mini-batches </span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> % </span><br><span class="line">                 (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[1,  2000] loss: 2.217
[1,  4000] loss: 1.855
[1,  6000] loss: 1.672
[1,  8000] loss: 1.570
[1, 10000] loss: 1.506
[1, 12000] loss: 1.461
[2,  2000] loss: 1.370
[2,  4000] loss: 1.369
[2,  6000] loss: 1.340
[2,  8000] loss: 1.323
[2, 10000] loss: 1.276
[2, 12000] loss: 1.279
Finished Training
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images </span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/Pytorch_Tutorial_output_42_0.png" alt=""></p>
<pre><code>GroundTruth:    cat  ship  ship plane
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br><span class="line">print(outputs)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[-0.7807, -1.7329,  0.9516,  1.8043, -1.2077,  0.7349,  1.3615, -1.5412,
          1.0311, -1.7234],
        [ 5.2413,  6.1315, -1.7400, -3.2287, -5.0497, -5.8038, -4.2375, -4.4465,
          7.7529,  4.1842],
        [ 2.7686,  3.8909, -0.7050, -1.7185, -3.2625, -3.2960, -2.2888, -2.6324,
          3.8761,  2.4697],
        [ 4.0929,  1.8480,  0.1962, -1.7907, -1.6931, -3.4538, -2.2210, -3.0694,
          4.5059,  0.7960]], grad_fn=&lt;ThAddmmBackward&gt;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">print(predicted)</span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([3, 8, 1, 8])
Predicted:    cat  ship   car  ship
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line"><span class="comment">#         print(labels)</span></span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of the network on the 10000 test images: 54 %
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>) <span class="comment"># torch.max 可以返回最大值和对应的坐标，np.random.randn只能返回最大值</span></span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of plane : 55 %
Accuracy of   car : 59 %
Accuracy of  bird : 65 %
Accuracy of   cat : 29 %
Accuracy of  deer : 22 %
Accuracy of   dog : 49 %
Accuracy of  frog : 71 %
Accuracy of horse : 55 %
Accuracy of  ship : 75 %
Accuracy of truck : 62 %
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/03/Multimodal-Machine-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HJY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HJY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/03/Multimodal-Machine-Learning/" itemprop="url">多模态机器学习总结与多模态医学图像处理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-03T17:25:45+08:00">
                2018-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习/" itemprop="url" rel="index">
                    <span itemprop="name">学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="多模态机器学习"><a href="#多模态机器学习" class="headerlink" title="多模态机器学习"></a>多模态机器学习</h1><p>下面是我参考文章[1]总结出来多模态机器学习中存在的挑战，以及目前所使用的方法的思维导图。<br><img src="https://raw.githubusercontent.com/hjyai94/Blog/master/source/uploads/Multimodal%20Machine%20Learning.gif" alt=""></p>
<h1 id="多模态医学图像数据集"><a href="#多模态医学图像数据集" class="headerlink" title="多模态医学图像数据集"></a>多模态医学图像数据集</h1><ol>
<li><p>脑肿瘤分割数据集<br>[1] Menze B H, Jakab A, Bauer S, et al. The multimodal brain tumor image segmentation benchmark (BRATS)[J]. IEEE transactions on medical imaging, 2015, 34(10): 1993.<br><a href="https://www.med.upenn.edu/sbia/brats2018.html" target="_blank" rel="noopener">https://www.med.upenn.edu/sbia/brats2018.html</a><br>这个数据集是多模态核磁成像数据集，对肿瘤进行不同尺度的扫描，然后进行多模态分割。</p>
</li>
<li><p>融合数据库<br><a href="http://www.med.harvard.edu/AANLIB/" target="_blank" rel="noopener">http://www.med.harvard.edu/AANLIB/</a><br><a href="http://www.metapix.de/" target="_blank" rel="noopener">http://www.metapix.de/</a></p>
</li>
<li><p>IXI数据库<br><a href="http://brain-development.org/ixi-dataset/" target="_blank" rel="noopener">http://brain-development.org/ixi-dataset/</a><br>这个数据库包含600张MRI图像，可以用于预训练，下面的文章使用了在这个方法。<br>Simonovsky M, Gutiérrez-Becker B, Mateus D, et al. A deep metric for multimodal registration[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2016: 10-18.</p>
</li>
<li><p>Radiology Objects in COntext<br><a href="https://github.com/razorx89/roco-dataset" target="_blank" rel="noopener">https://github.com/razorx89/roco-dataset</a><br>可以用于训练生成模型生成图片标题，用于分类图片的分类模型，还有基于内容的图像检索。</p>
</li>
<li><p>Grand Challenges in Biomedical Image Analysis<br><a href="https://grand-challenge.org/" target="_blank" rel="noopener">https://grand-challenge.org/</a><br>这个网站是对已经发表的论文或者是目前使用的算法提供一个竞赛的平台，里面有很多公开的医学图像数据集，其中包括1， 6， 7中的数据集。</p>
</li>
<li><p>Motion Tracking Challenge<br><a href="http://stacom.cardiacatlas.org/motion-tracking-challenge/" target="_blank" rel="noopener">http://stacom.cardiacatlas.org/motion-tracking-challenge/</a><br>这个数据库中包含有MRI和3D ultrasound 两个模态的图像。</p>
</li>
<li><p>​Automatic Intervertebral Disc Localization and Segmentation from 3D Multi-modality MR (M3) Images<br><a href="https://ivdm3seg.weebly.com/data.html" target="_blank" rel="noopener">https://ivdm3seg.weebly.com/data.html</a><br>这个数据集是MRI图像，用于定位和分割。</p>
</li>
<li><p>一些数据集<br><a href="https://sites.google.com/site/aacruzr/image-datasets" target="_blank" rel="noopener">https://sites.google.com/site/aacruzr/image-datasets</a></p>
</li>
<li><p>Image Registration Evaluation Project<br><a href="http://www.insight-journal.org/rire/" target="_blank" rel="noopener">http://www.insight-journal.org/rire/</a><br>用作image registratration效果评价的数据集</p>
</li>
<li><p>DICOM image sample sets<br>有一些数据供下载，部分是多模态的，数量比较少，只是一些例子，另外，看起来是注册是要收费的样子。</p>
</li>
<li><p><a href="https://ida.loni.usc.edu/login.jsp" target="_blank" rel="noopener">https://ida.loni.usc.edu/login.jsp</a><br>一些脑神经科学的医学图像数据库</p>
</li>
<li><p>ANDI(阿尔兹海默症)<br><a href="http://adni.loni.usc.edu/data-samples/access-data/" target="_blank" rel="noopener">http://adni.loni.usc.edu/data-samples/access-data/</a></p>
</li>
<li><p>Cancer Imaging Archive<br><a href="http://www.cancerimagingarchive.net/" target="_blank" rel="noopener">http://www.cancerimagingarchive.net/</a></p>
</li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] Baltrušaitis T, Ahuja C, Morency L P. Multimodal machine learning: A survey and taxonomy[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="HJY" />
            
              <p class="site-author-name" itemprop="name">HJY</p>
              <p class="site-description motion-element" itemprop="description">HJY的装逼小站</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/hjyai94" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HJY</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.3"></script>



  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
